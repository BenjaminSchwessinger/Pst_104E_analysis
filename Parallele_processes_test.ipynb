{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will need\n",
    "import subprocess  # The library that will actually be talking to the shell and\n",
    "                   # tell it to what to run and when.\n",
    "from itertools import islice  # Important tool that will allow us to split up our\n",
    "                              # commands for each output.\n",
    "import random  # This will determine how long each process will take.\n",
    "random.seed(1)  # Feel free to change this, but useful in the notebook so the author can explain\n",
    "                # the output even if the output is 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='output.file.0' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='output.file.1' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='output.file.2' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='output.file.3' mode='w' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='output.file.4' mode='w' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "# Set the number of threads\n",
    "threads = 5  # Of the 200 commands, five will be running at any one time.\n",
    "\n",
    "# We need to have 5 separate output files to stop each running command from \n",
    "# over writing the work of a simultaneous command.\n",
    "output_files = [\"output.file.%d\" % i for i in range(0, threads)]  #output.file.0 to output.file.4\n",
    "\n",
    "file_handlers = [None]*threads  # Generates a list of NULL variables of length 5.\n",
    "\n",
    "# This assigns the file handler for each file.\n",
    "for index, output_file in enumerate(output_files):\n",
    "    file_handlers[index] = open(output_file, 'w')\n",
    "\n",
    "for handler in file_handlers:  # Print the file handler so we know what they look like.\n",
    "    print (handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_number_list = [random.uniform(0.1,1) for i in range(0,10)]\n",
    "commands = [\"sleep %d && echo Command number - %d. Slept for %f.\" % (j, i, j)\n",
    "            for i, j in enumerate(random_number_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processes = (subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            for cmd in commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processes = (cmd for cmd in commands)\n",
    "running_processes = list(islice(processes, threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleep 0 && echo Command number - 5. Slept for 0.111144.\n",
      "0\n",
      "what is running_processes[i]? it is None\n",
      "sleep 0 && echo Command number - 6. Slept for 0.703370.\n",
      "1\n",
      "what is running_processes[i]? it is None\n",
      "sleep 0 && echo Command number - 7. Slept for 0.182515.\n",
      "2\n",
      "what is running_processes[i]? it is None\n",
      "sleep 0 && echo Command number - 8. Slept for 0.203592.\n",
      "3\n",
      "what is running_processes[i]? it is None\n",
      "sleep 0 && echo Command number - 9. Slept for 0.896554.\n",
      "4\n",
      "what is running_processes[i]? it is None\n"
     ]
    }
   ],
   "source": [
    "for i, process in enumerate(running_processes):\n",
    "    print(process)\n",
    "    print(i)\n",
    "    running_processes[i] = next(processes, None) #this line moves along the content of the running_processes list one at a time \n",
    "    #pulling \n",
    "    print('what is running_processes[i]? it is %s' % (running_processes[i]))\n",
    "    #running_processes[i] = next(processes, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "running_processes = islice(processes, threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "running_processes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while running_processes:\n",
    "    for i, process in enumerate(running_processes):\n",
    "        #print(i)\n",
    "        if process.poll() is not None:  # Means that the process is complete!\n",
    "            stdout, stderr = process.communicate()  # Get the output of the completed process\n",
    "            file_handlers[i].write(str(stdout) + \"\\n\")  # Write the output to handler that\n",
    "            running_processes[i] = next(processes, None)\n",
    "            # Run the next number in the list.\n",
    "            if running_processes[i] is None:  # No more commands waiting to be processed.\n",
    "                del running_processes[i]  # Not a valid process.\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# By closing the file_handler this prints everything accumulated in the handler to the file.\n",
    "for handler in file_handlers:\n",
    "    handler.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's have a look at the first few lines of each each file.\n",
    "number_of_lines = 3\n",
    "for output_file in output_files:\n",
    "    with open(output_file) as output_handler:\n",
    "        head = list(islice(output_handler, number_of_lines))\n",
    "    print (\"### \" + output_file + \" ###\")\n",
    "    print (head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Set playing around with multiprocessing using the TE files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "import pysam\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SearchIO\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pybedtools\n",
    "import multiprocessing\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genome = 'Pst_104E_v12_p_ctg'\n",
    "p_genome_file = 'Pst_104E_v12_p_ctg.genome_file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dir = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/TE_analysis'\n",
    "TE_post_analysis_p = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/REPET/Pst79_p/Pst79_p_full_annotate/postanalysis'\n",
    "TE_post_analysis_p_header = 'TE      length  covg    frags   fullLgthFrags   copies  fullLgthCopies  meanId  sdId    minId   q25Id   medId   q75Id   maxId   meanLgth        sdLgth  minLgth q25Lgth medLgth q75Lgth maxLgth meanLgthPerc    sdLgthPerc      minLgthPerc  q25LgthPerc     medLgthPerc     q75LgthPerc     maxLgthPerc'.split(' ')\n",
    "TE_post_analysis_p_header = [x for x in TE_post_analysis_p_header if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the directory structure to safe specific coverage files\n",
    "os.chdir(out_dir)\n",
    "TE_types = ['Retrotransposon', 'DNA_transposon', 'noCat', 'SSR']\n",
    "TE_path = [os.path.join(out_dir, x) for x in TE_types]\n",
    "TE_path_dict = dict(zip(TE_types, TE_path))\n",
    "for TE_type in TE_types:\n",
    "    new_path = os.path.join(out_dir, TE_type)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.mkdir(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this needs to be fixed up to pick the proper summary table\n",
    "p_repet_summary_df = pd.read_csv(TE_post_analysis_p+'/'+'Pst79p_anno_chr_allTEs_nr_noSSR_join_path.annotStatsPerTE.tab' ,\\\n",
    "                                names = TE_post_analysis_p_header, header=None, sep='\\t', skiprows=1 )\n",
    "\n",
    "#check if I can filter the tab files for removing all TEs that are on the 2000 plus contigs\n",
    "#remove tRNAs TEs with infernal\n",
    "\n",
    "p_repet_summary_df['Code'] = p_repet_summary_df['TE'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "code_keys = p_repet_summary_df['Code'].unique()\n",
    "\n",
    "code_keys.sort()\n",
    "\n",
    "code_long = ['DNA_transposon Helitron', 'DNA_transposon Helitron', 'DNA_transposon Helitron', 'DNA_transposon Maverick',\\\n",
    "            'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon noCat',\\\n",
    "             'DNA_transposon MITE','DNA_transposon MITE', 'Potential Host Gene', 'Retrotransposon LINE', 'Retrotransposon LINE',\\\n",
    "             'Retrotransposon LINE','Retrotransposon LTR','Retrotransposon LTR', 'Retrotransposon LTR', 'Retrotransposon LTR', 'Retrotransposon PLE', \\\n",
    "             'Retrotransposon SINE',  'Retrotransposon SINE', 'Retrotransposon noCat', 'Retrotransposon LARD',\\\n",
    "             'Retrotransposon LARD', 'Retrotransposon TRIM', 'Retrotransposon TRIM', 'Retrotransposon noCat',  \\\n",
    "             'Retrotransposon DIRS','Retrotransposon DIRS','Retrotransposon DIRS','Retrotransposon DIRS',\\\n",
    "             'noCat', 'noCat']\n",
    "\n",
    "code_dict = dict(zip(code_keys, code_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48659\n"
     ]
    }
   ],
   "source": [
    "REPET_ID_df = pd.read_csv(out_dir+'/'+genome+'.REPET.ID_column.gff', header=None, sep='\\t')\n",
    "_id = pd.read_csv(out_dir+'/'+genome+'.REPET.ID_column.gff', header=None, sep='\\t')[8].unique()\n",
    "REPET_ID_bed = pybedtools.BedTool(out_dir+'/'+genome+'.REPET.ID_column.gff')\n",
    "REPET_ID_bed = REPET_ID_bed.remove_invalid().saveas(out_dir+'/'+genome+'.REPET.ID_column.bedobject')\n",
    "print(len(_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1494"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(REPET_ID_df[REPET_ID_df[8].str.contains(\"MCL\")][8].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we create a function to pass only features for a particular\n",
    "# featuretype.  This is similar to a \"grep\" operation when applied to every\n",
    "# feature in a BedTool\n",
    "def id_filter(feature, _id):\n",
    "    if feature[8] == _id:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id_m(_id_list, _dict):\n",
    "    repet_prefix = 'Pst_104E_v12_p_ctg.REPET.TE_level'\n",
    "    for _id in _id_list:\n",
    "    \n",
    "        #ClassI are retrotransposon form blast\n",
    "        if 'ClassI:' in _id:\n",
    "            out_path = TE_path_dict['Retrotransposon']   \n",
    "        #ClassII are DNA_transponson\n",
    "        elif 'ClassII' in _id:\n",
    "            out_path = TE_path_dict['DNA_transposon'] \n",
    "        #The rest with '_' should be REPET_TEs\n",
    "        elif _id.split('_')[0] in list(code_dict.keys()):\n",
    "            key = code_dict[_id.split('_')[0]].split(' ')[0]\n",
    "            out_path = TE_path_dict[key]\n",
    "        #everything without '_' at the end should be SSR\n",
    "        elif '_' not in _id:\n",
    "            out_path = TE_path_dict['SSR']\n",
    "        out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "        result = REPET_ID_bed.filter(id_filter, _id).saveas(out_fn)\n",
    "        cov_fn = out_fn.replace('gff','cov')\n",
    "        cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "        cov.saveas(cov_fn)\n",
    "        _len = len(pd.read_csv(cov_fn, header=None, sep='\\t'))\n",
    "        _dict[_id] = _len\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-0ac9e902af00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mrunning_process\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Waiting for Subset_TE_level classificatio to finish!\"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[0mrunning_process\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All Subset_id_classifications done! Totaling %i\"\u001b[0m\u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mjob_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "import math\n",
    "nproces = 30 #number of processors\n",
    "chunksize = int(math.ceil(len(_id) / float(nproces)))\n",
    "jobs = []\n",
    "job_count = 0\n",
    "manager = multiprocessing.Manager()\n",
    "TE_cov_df  = manager.dict()\n",
    "for i in range(nproces):\n",
    "        p = multiprocessing.Process(target=subset_id_m, args=(_id[chunksize * i:chunksize * (i + 1)],TE_cov_df))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        job_count += 1\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    running_process = [j.is_alive() for j in jobs].count(True)\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_TE_level classificatio to finish!\"% running_process)\n",
    "print(\"All Subset_id_classifications done! Totaling %i\"% (job_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n",
      "Waiting for Subset_TE_level classificatio to finish! 30 jobs still running\n"
     ]
    }
   ],
   "source": [
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    running_process = [j.is_alive() for j in jobs].count(True)\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_TE_level classificatio to finish! %i jobs still running\"% running_process)\n",
    "print(\"All Subset_id_classifications done! Totaling %i\"% (job_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
