{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/Bio/SearchIO/__init__.py:211: BiopythonExperimentalWarning: Bio.SearchIO is an experimental submodule which may undergo significant changes prior to its future official release.\n",
      "  BiopythonExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "import pysam\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SearchIO\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pybedtools\n",
    "import multiprocessing\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ID_filter_gff(_feature, _id):\n",
    "    \"\"\"\n",
    "    This filter parses out the top level id form the 9th gff column form a REPET gff file.\n",
    "    It has a specific search pattern for each feature type in column 2.\n",
    "    _type is defined by the feature '_'.join(feature.split(\"_\")[-2:])\n",
    "    \"\"\"\n",
    "    _type = '_'.join(_feature.split(\"_\")[-2:])\n",
    "    if _type == 'REPET_TEs':\n",
    "\n",
    "        TE_pattern = r'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([^;| ]*)'\n",
    "        TE_prog = re.compile(TE_pattern)\n",
    "        TE_match = TE_prog.search(_id)\n",
    "\n",
    "        try:\n",
    "            return TE_match.group(1)\n",
    "        except AttributeError:\n",
    "            print(_id)\n",
    "\n",
    "    if _type == 'REPET_SSRs':\n",
    "        \n",
    "        SSR_pattern = 'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([A-Z,a-z,0-9,-]*)'\n",
    "        SSR_prog = re.compile(SSR_pattern)\n",
    "        SSR_match = SSR_prog.search(_id)\n",
    "        return SSR_match.group(1)\n",
    "    if _type == 'REPET_tblastx' or _type == 'REPET_blastx':\n",
    "        #if \"#\" in _id:\n",
    "        #     blast_pattern = 'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([A-Z,a-z,0-9,-]*[_]?[A-Z,a-z,0-9,-]*[_|#|0-9]+?:[A-Z,a-z,0-9,-,:]*)'\n",
    "        #else:\n",
    "        #    blast_pattern = r'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([\\w+|:|-]*)'\n",
    "        blast_prog = re.compile(r'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([^;| ]*)')\n",
    "        #blast_prog = re.compile(blast_pattern)\n",
    "        blast_match = blast_prog.search(_id)\n",
    "        return blast_match.group(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blast_hit_gff(_feature, _row8, _id):\n",
    "    \"\"\"\n",
    "    This filter parses the blast hit for REPET_TEs from the new 'ID' column. If no blast hit available returns Pastec ids.\n",
    "    If the result is blast already the value is simple parse the blast hit.\n",
    "    SSRs also get SSR\n",
    "    !!!Requires the three_letter_dict to be defined previously.!!!\n",
    "    _type is defined by the feature '_'.join(feature.split(\"_\")[-2:])\n",
    "    \"\"\"\n",
    "    _type = '_'.join(_feature.split(\"_\")[-2:])\n",
    "    if _type == 'REPET_TEs':\n",
    "        #split the pastec_cat into the first three letter code\n",
    "        pastec_cat = _id.split('_')[0]\n",
    "        if 'TE_BLR' in _row8:\n",
    "            #hit_list = [x.split(';')[3] for x in _row8]\n",
    "            blast_hit_pattern = r'TE_BLR\\w*: (\\S*)[ |;]'\n",
    "            blast_hit_prog = re.compile(blast_hit_pattern)\n",
    "            TE_match = blast_hit_prog.findall(_row8)\n",
    "            first_sub_class = ':'.join(TE_match[0][:-1].split(':')[1:])\n",
    "            if len([x for x in TE_match if first_sub_class in x]) == len(TE_match):\n",
    "                if ';' in first_sub_class:\n",
    "                    return first_sub_class.split(';')[0]\n",
    "                else:\n",
    "                    return first_sub_class\n",
    "#fix this here to include the there letter code of the first bit of the ID similar to the blast hits\n",
    "#e.g. ClassI:?:? and so on. a dict might be the easiest here.\n",
    "            \n",
    "            else:\n",
    "                return three_letter_dict[pastec_cat]\n",
    "        else:\n",
    "            return three_letter_dict[pastec_cat]\n",
    "    if _type == 'REPET_SSRs':\n",
    "        return 'SSR'\n",
    "        \n",
    "\n",
    "        return SSR_match.group(1)\n",
    "    if _type == 'REPET_tblastx' or _type == 'REPET_blastx':\n",
    "        return ':'.join(_id.split(':')[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TE_classification_filter(_id, level = 0):\n",
    "    \"\"\"\n",
    "    This function pulls out the class == level1, Order == level2, Superfamily == leve3.\n",
    "    If SSR or noCat return these values.\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(_id.split(':')) == 1:\n",
    "        return _id\n",
    "    if level == 0:\n",
    "        _class = _id.split(':')[0]\n",
    "        if _class == 'ClassI':\n",
    "            return 'Retrotransposon'\n",
    "        if _class == 'ClassII':\n",
    "            return 'DNA_transposon'\n",
    "    elif level == 1:\n",
    "        _order = _id.split(':')[1]\n",
    "        if _order == '?':\n",
    "            return 'noCat'\n",
    "        else:\n",
    "            return _order\n",
    "    elif level == 2:\n",
    "        _superfamily = _id.split(':')[2]\n",
    "        if _superfamily == '?':\n",
    "            return 'noCat'\n",
    "        else:\n",
    "            return _superfamily\n",
    "    else:\n",
    "        print('Something wrong! Check if level is 0, 1 or 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_dir = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genome = 'Pst_104E_v12_p_ctg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dir = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/TE_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove all commenting lines from the initial repet file\n",
    "!grep -v \"^#\" {source_dir}/{genome}.REPET.gff > {out_dir}/{genome}.REPET.gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff = pd.read_csv(out_dir+'/'+genome+'.REPET.gff', sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_post_analysis_p = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/REPET/Pst79_p/Pst79_p_full_annotate/postanalysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_post_analysis_p_header = 'TE      length  covg    frags   fullLgthFrags   copies  fullLgthCopies  meanId  sdId    minId   q25Id   medId   q75Id   maxId   meanLgth        sdLgth  minLgth q25Lgth medLgth q75Lgth maxLgth meanLgthPerc    sdLgthPerc      minLgthPerc  q25LgthPerc     medLgthPerc     q75LgthPerc     maxLgthPerc'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_post_analysis_p_header = [x for x in TE_post_analysis_p_header if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this needs to be fixed up to pick the proper summary table\n",
    "p_repet_summary_df = pd.read_csv(TE_post_analysis_p+'/'+'Pst79p_anno_chr_allTEs_nr_noSSR_join_path.annotStatsPerTE.tab' ,\\\n",
    "                                names = TE_post_analysis_p_header, header=None, sep='\\t', skiprows=1 )\n",
    "\n",
    "#check if I can filter the tab files for removing all TEs that are on the 2000 plus contigs\n",
    "#remove tRNAs TEs with infernal\n",
    "\n",
    "p_repet_summary_df['Code'] = p_repet_summary_df['TE'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "code_keys = p_repet_summary_df['Code'].unique()\n",
    "\n",
    "code_keys.sort()\n",
    "\n",
    "code_long = ['DNA_transposon Helitron', 'DNA_transposon Helitron', 'DNA_transposon Helitron', 'DNA_transposon Maverick',\\\n",
    "            'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon noCat',\\\n",
    "             'DNA_transposon MITE','DNA_transposon MITE', 'Potential Host Gene', 'Retrotransposon LINE', 'Retrotransposon LINE',\\\n",
    "             'Retrotransposon LINE','Retrotransposon LTR','Retrotransposon LTR', 'Retrotransposon LTR', 'Retrotransposon LTR', 'Retrotransposon PLE', \\\n",
    "             'Retrotransposon SINE',  'Retrotransposon SINE', 'Retrotransposon noCat', 'Retrotransposon LARD',\\\n",
    "             'Retrotransposon LARD', 'Retrotransposon TRIM', 'Retrotransposon TRIM', 'Retrotransposon noCat',  \\\n",
    "             'Retrotransposon DIRS','Retrotransposon DIRS','Retrotransposon DIRS','Retrotransposon DIRS',\\\n",
    "             'noCat', 'noCat']\n",
    "\n",
    "code_dict = dict(zip(code_keys, code_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>copies</th>\n",
       "      <th>covg</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Code long</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DNA_transposon Helitron</th>\n",
       "      <td>1075</td>\n",
       "      <td>817566</td>\n",
       "      <td>2989.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNA_transposon MITE</th>\n",
       "      <td>3789</td>\n",
       "      <td>886304</td>\n",
       "      <td>490.240741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNA_transposon Maverick</th>\n",
       "      <td>268</td>\n",
       "      <td>345406</td>\n",
       "      <td>8562.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNA_transposon TIR</th>\n",
       "      <td>19166</td>\n",
       "      <td>12711595</td>\n",
       "      <td>4020.474359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DNA_transposon noCat</th>\n",
       "      <td>5286</td>\n",
       "      <td>2376456</td>\n",
       "      <td>3034.734513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Potential Host Gene</th>\n",
       "      <td>1372</td>\n",
       "      <td>1375304</td>\n",
       "      <td>6120.490566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrotransposon DIRS</th>\n",
       "      <td>1337</td>\n",
       "      <td>1049299</td>\n",
       "      <td>6874.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrotransposon LARD</th>\n",
       "      <td>10752</td>\n",
       "      <td>4947564</td>\n",
       "      <td>5407.736111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrotransposon LINE</th>\n",
       "      <td>323</td>\n",
       "      <td>237992</td>\n",
       "      <td>4446.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrotransposon LTR</th>\n",
       "      <td>18893</td>\n",
       "      <td>16276421</td>\n",
       "      <td>6384.627841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrotransposon PLE</th>\n",
       "      <td>75</td>\n",
       "      <td>41843</td>\n",
       "      <td>8954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrotransposon SINE</th>\n",
       "      <td>163</td>\n",
       "      <td>31138</td>\n",
       "      <td>317.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrotransposon TRIM</th>\n",
       "      <td>507</td>\n",
       "      <td>238695</td>\n",
       "      <td>1583.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retrotransposon noCat</th>\n",
       "      <td>112</td>\n",
       "      <td>101098</td>\n",
       "      <td>4365.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noCat</th>\n",
       "      <td>1218</td>\n",
       "      <td>850828</td>\n",
       "      <td>1679.829268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         copies      covg       length\n",
       "Code long                                             \n",
       "DNA_transposon Helitron    1075    817566  2989.764706\n",
       "DNA_transposon MITE        3789    886304   490.240741\n",
       "DNA_transposon Maverick     268    345406  8562.666667\n",
       "DNA_transposon TIR        19166  12711595  4020.474359\n",
       "DNA_transposon noCat       5286   2376456  3034.734513\n",
       "Potential Host Gene        1372   1375304  6120.490566\n",
       "Retrotransposon DIRS       1337   1049299  6874.526316\n",
       "Retrotransposon LARD      10752   4947564  5407.736111\n",
       "Retrotransposon LINE        323    237992  4446.625000\n",
       "Retrotransposon LTR       18893  16276421  6384.627841\n",
       "Retrotransposon PLE          75     41843  8954.000000\n",
       "Retrotransposon SINE        163     31138   317.923077\n",
       "Retrotransposon TRIM        507    238695  1583.434783\n",
       "Retrotransposon noCat       112    101098  4365.857143\n",
       "noCat                      1218    850828  1679.829268"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_repet_summary_df['Code long'] = p_repet_summary_df['Code'].apply(lambda x: code_dict[x])\n",
    "p_repet_summary_sum_df = pd.pivot_table(p_repet_summary_df, values=['covg', 'copies'], index='Code long', aggfunc=np.sum)\n",
    "p_repet_summary_mean_df = pd.pivot_table(p_repet_summary_df, values='length', index='Code long', aggfunc=np.mean)\n",
    "pd.concat([p_repet_summary_sum_df,p_repet_summary_mean_df], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now filter the gff dataframe to delete all the high coverage contigs\n",
    "#This might would have to be fixed as well. If we don't delete it as files should be already filtered\n",
    "contigs_smaller_2000 = pd.read_csv('/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly/pcontig_smaller_2000.txt',\\\n",
    "                                  header=None)[0].tolist()\n",
    "\n",
    "p_repet_gff = pd.read_csv(out_dir+'/'+genome+'.REPET.gff', sep='\\t', header = None)\n",
    "\n",
    "p_repet_gff_filtered = p_repet_gff[p_repet_gff[0].isin(contigs_smaller_2000)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter out potential host genes\n",
    "p_repet_gff_filtered = p_repet_gff_filtered[~p_repet_gff_filtered[8].str.contains(\"Potential\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered['ID'] = p_repet_gff_filtered.apply(lambda row: ID_filter_gff(row[1], row[8]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#re-generate the code dict using the gff ID as Code keys\n",
    "\n",
    "\n",
    "code_keys = p_repet_gff_filtered[p_repet_gff_filtered[1].str.contains('REPET_TE')]['ID'].unique()\n",
    "\n",
    "code_keys = list({x.split('_')[0] for x in code_keys})\n",
    "\n",
    "code_keys.sort()\n",
    "\n",
    "code_long = ['DNA_transposon Helitron', 'DNA_transposon Helitron', 'DNA_transposon Helitron', 'DNA_transposon Maverick',\\\n",
    "            'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon noCat',\\\n",
    "             'DNA_transposon MITE','DNA_transposon MITE', 'Potential Host Gene', 'Retrotransposon LINE', 'Retrotransposon LINE',\\\n",
    "             'Retrotransposon LINE','Retrotransposon LTR','Retrotransposon LTR', 'Retrotransposon LTR', 'Retrotransposon LTR', 'Retrotransposon PLE', \\\n",
    "             'Retrotransposon SINE',  'Retrotransposon SINE', 'Retrotransposon noCat', 'Retrotransposon LARD',\\\n",
    "             'Retrotransposon LARD', 'Retrotransposon TRIM', 'Retrotransposon TRIM', 'Retrotransposon noCat',  \\\n",
    "             'Retrotransposon DIRS','Retrotransposon DIRS','Retrotransposon DIRS','Retrotransposon DIRS',\\\n",
    "             'noCat', 'noCat']\n",
    "\n",
    "if len(code_keys) == len(code_long):\n",
    "    print(\"should be okay!\")\n",
    "\n",
    "code_dict = dict(zip(code_keys, code_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aas\n"
     ]
    }
   ],
   "source": [
    "three_letter_code = list({x for x in code_keys})\n",
    "\n",
    "three_letter_code.sort()\n",
    "\n",
    "three_letter_values = []\n",
    "for x in three_letter_code:\n",
    "    if 'MITE' in x:\n",
    "        _value = \"ClassII:MITE:?\"\n",
    "        three_letter_values.append(_value)\n",
    "        continue\n",
    "    if 'LARD' in x:\n",
    "        _value = 'ClassI:LARD:?'\n",
    "        three_letter_values.append(_value)\n",
    "        continue\n",
    "    if 'TRIM' in x:\n",
    "        _value = 'ClassI:TRIM:?'\n",
    "        three_letter_values.append(_value)\n",
    "        continue\n",
    "    _value =''\n",
    "    if x[0] == 'D':\n",
    "        _value = _value + 'ClassII:'\n",
    "    if x[0] == 'R':\n",
    "        _value = _value + 'ClassI:'\n",
    "    if x[0] != 'D' and x[0] != 'R':\n",
    "        _value = 'noCat'\n",
    "        three_letter_values.append(_value)\n",
    "        continue\n",
    "    if x[1] == 'T':\n",
    "        _value = _value + 'TIR:?'\n",
    "    if x[1] == 'H':\n",
    "        _value = _value + 'Helitron:?'\n",
    "    if x[1] == 'M':\n",
    "        _value = _value + 'Maverick:?'\n",
    "    if x[0:2] == 'DY':\n",
    "        _value = _value + ':Crypton:?'\n",
    "    if x[1] == 'X':\n",
    "        _value = _value + '?:?'\n",
    "    if x[1] == 'I':\n",
    "        _value = _value + 'LINE:?'\n",
    "    if x[1] == 'L':\n",
    "        _value = _value + 'LTR:?'\n",
    "    if x[1] == 'P':\n",
    "        _value = _value + 'Penelope:?'\n",
    "    if x[1] == 'S':\n",
    "        _value = _value + 'SINE:?'\n",
    "    if x[0:2] == 'RY':\n",
    "        _value = _value + 'DIRS:?'    \n",
    "    three_letter_values.append(_value)\n",
    "\n",
    "if len(three_letter_code) == len(three_letter_values):\n",
    "    print(\"Aas\")\n",
    "    three_letter_dict = dict(zip(three_letter_code, three_letter_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered['Class:Order:Superfamily'] = p_repet_gff_filtered.apply(lambda row: blast_hit_gff(row[1], row[8], row['ID']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassII:?:Ginger2/TDD\n",
      "ClassII:?:Ginger2_TDD\n"
     ]
    }
   ],
   "source": [
    "#generate a dict that can be used to rename the Class:Order:Superfamily column considering that partial matches ([2] == match_part) might contain different\n",
    "#IDs even though they are the same TE only partial.\n",
    "_tmp_subset = p_repet_gff_filtered[~p_repet_gff_filtered[1].str.contains('SSR')].loc[:, 'ID':].sort_values(by=['ID','Class:Order:Superfamily'])\\\n",
    ".drop_duplicates(subset='ID', keep ='last')\n",
    "\n",
    "TE_COS_dict = dict(zip(_tmp_subset.loc[:, 'ID'], _tmp_subset.loc[:, 'Class:Order:Superfamily' ]))\n",
    "\n",
    "_tmp_subset = p_repet_gff_filtered[p_repet_gff_filtered[1].str.contains('SSR')].loc[:, 'ID':].sort_values(by=['ID','Class:Order:Superfamily'])\\\n",
    ".drop_duplicates(subset='ID', keep ='last')\n",
    "\n",
    "_tmp_dict = dict(zip(_tmp_subset.loc[:, 'ID'], _tmp_subset.loc[:, 'Class:Order:Superfamily' ]))\n",
    "\n",
    "TE_COS_dict.update(_tmp_dict)\n",
    "#remove all backslashes from the values as this will conflict with the output later on\n",
    "for x in TE_COS_dict.keys():\n",
    "    if '/' in TE_COS_dict[x]:\n",
    "        value = TE_COS_dict[x]\n",
    "        print(value)\n",
    "        TE_COS_dict[x] = value.replace('/','_')\n",
    "        print(TE_COS_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered.to_csv(out_dir+'/'+genome+'.REPET.long.df', sep='\\t', header = None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered['Class:Order:Superfamily'] = p_repet_gff_filtered['ID'].apply(lambda x: TE_COS_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the unique Class:Order:Superfamily classifiers of this dataframe:\n",
      "['ClassII:TIR:hAT' 'ClassII:TIR:CACTA' 'ClassI:LTR:Gypsy'\n",
      " 'ClassII:TIR:PIF-Harbinger' 'ClassI:LTR:Copia' 'SSR' 'ClassII:?:?'\n",
      " 'ClassII:MITE:?' 'ClassI:LTR:?' 'ClassI:LARD:?' 'ClassI:SINE:?'\n",
      " 'ClassII:TIR:Tc1-Mariner' 'ClassII:TIR:?' 'ClassII:TIR:MuDR' 'noCat'\n",
      " 'ClassII:Helitron:?' 'ClassII:TIR:P' 'ClassII:Helitron:Helitron'\n",
      " 'ClassI:LTR:ERV' 'ClassII:Maverick:?' 'ClassI:LINE:I' 'ClassII:?:Academ'\n",
      " 'ClassI:DIRS:DIRS' 'ClassII:?:Ginger2_TDD' 'ClassI:TRIM:?'\n",
      " 'ClassI:LINE:Jockey' 'ClassI:?:?' 'ClassII:?:Ginger1'\n",
      " 'ClassI:LTR:Retrovirus' 'ClassI:LTR:Bel-Pao' 'ClassII:Maverick:Maverick'\n",
      " 'ClassI:PLE:Penelope' 'ClassI:Penelope:?' 'ClassI:LINE:RTE'\n",
      " 'ClassI:LINE:?' 'ClassI:LINE:L1' 'ClassII:Crypton:Crypton'\n",
      " 'ClassII:?:Sola' 'ClassII:TIR:PiggyBac' 'ClassI:LINE:R2' 'ClassI:DIRS:?'\n",
      " 'ClassII:?:Kolobok' 'ClassII:TIR:Transib']\n"
     ]
    }
   ],
   "source": [
    "print('These are the unique Class:Order:Superfamily classifiers of this dataframe:')\n",
    "print(p_repet_gff_filtered['Class:Order:Superfamily'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#have a rough summary of the coverage not considering overlaps.\n",
    "p_repet_gff_filtered.drop_duplicates(subset=[3,4,'ID'], inplace =True)\n",
    "p_repet_gff_filtered['Length'] = p_repet_gff_filtered[4] - p_repet_gff_filtered[3]\n",
    "p_repet_gff_filtered['Class'] = p_repet_gff_filtered.apply(lambda row: TE_classification_filter(row['Class:Order:Superfamily'], 0), axis=1)\n",
    "p_repet_gff_filtered['Order'] = p_repet_gff_filtered.apply(lambda row: TE_classification_filter(row['Class:Order:Superfamily'], 1), axis=1)\n",
    "p_repet_gff_filtered['Superfamily'] = p_repet_gff_filtered.apply(lambda row: TE_classification_filter(row['Class:Order:Superfamily'], 2), axis=1)\n",
    "p_repet_gff_len_COS = p_repet_gff_filtered.groupby(by=['Class','Order','Superfamily'])['Length'].sum()\n",
    "p_repet_gff_len_S = p_repet_gff_filtered.groupby(by=['Class:Order:Superfamily'])['Length'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the summary of overlapping coverage according to Class, Order, Superfamily\n",
      "Class            Order     Superfamily  \n",
      "DNA_transposon   Crypton   Crypton              1980\n",
      "                 Helitron  Helitron          1402086\n",
      "                           noCat              773416\n",
      "                 MITE      noCat             1089424\n",
      "                 Maverick  Maverick            32983\n",
      "                           noCat              411043\n",
      "                 TIR       CACTA             2468455\n",
      "                           MuDR              4269984\n",
      "                           P                  273240\n",
      "                           PIF-Harbinger     4470021\n",
      "                           PiggyBac              345\n",
      "                           Tc1-Mariner       3164294\n",
      "                           Transib              1257\n",
      "                           hAT               8656260\n",
      "                           noCat             5176762\n",
      "                 noCat     Academ            2069193\n",
      "                           Ginger1              5331\n",
      "                           Ginger2_TDD            34\n",
      "                           Kolobok              1317\n",
      "                           Sola                 7549\n",
      "                           noCat             8367989\n",
      "Retrotransposon  DIRS      DIRS              1288040\n",
      "                           noCat                5222\n",
      "                 LARD      noCat             3354762\n",
      "                 LINE      I                  886166\n",
      "                           Jockey              40186\n",
      "                           L1                   2500\n",
      "                           R2                   3941\n",
      "                           RTE                  2767\n",
      "                           noCat                5056\n",
      "                 LTR       Bel-Pao              4378\n",
      "                           Copia            12553952\n",
      "                           ERV                140257\n",
      "                           Gypsy            29374783\n",
      "                           Retrovirus          29929\n",
      "                           noCat             3003699\n",
      "                 PLE       Penelope              730\n",
      "                 Penelope  noCat               74409\n",
      "                 SINE      noCat               32459\n",
      "                 TRIM      noCat              207950\n",
      "                 noCat     noCat              151904\n",
      "SSR              SSR       SSR               1923327\n",
      "noCat            noCat     noCat             1176049\n",
      "Name: Length, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the summary of overlapping coverage according to Class, Order, Superfamily\")\n",
    "print(p_repet_gff_len_COS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the summary of overlapping coverage according to Superfamily\n",
      "Class:Order:Superfamily\n",
      "ClassI:?:?                     151904\n",
      "ClassI:DIRS:?                    5222\n",
      "ClassI:DIRS:DIRS              1288040\n",
      "ClassI:LARD:?                 3354762\n",
      "ClassI:LINE:?                    5056\n",
      "ClassI:LINE:I                  886166\n",
      "ClassI:LINE:Jockey              40186\n",
      "ClassI:LINE:L1                   2500\n",
      "ClassI:LINE:R2                   3941\n",
      "ClassI:LINE:RTE                  2767\n",
      "ClassI:LTR:?                  3003699\n",
      "ClassI:LTR:Bel-Pao               4378\n",
      "ClassI:LTR:Copia             12553952\n",
      "ClassI:LTR:ERV                 140257\n",
      "ClassI:LTR:Gypsy             29374783\n",
      "ClassI:LTR:Retrovirus           29929\n",
      "ClassI:PLE:Penelope               730\n",
      "ClassI:Penelope:?               74409\n",
      "ClassI:SINE:?                   32459\n",
      "ClassI:TRIM:?                  207950\n",
      "ClassII:?:?                   8367989\n",
      "ClassII:?:Academ              2069193\n",
      "ClassII:?:Ginger1                5331\n",
      "ClassII:?:Ginger2_TDD              34\n",
      "ClassII:?:Kolobok                1317\n",
      "ClassII:?:Sola                   7549\n",
      "ClassII:Crypton:Crypton          1980\n",
      "ClassII:Helitron:?             773416\n",
      "ClassII:Helitron:Helitron     1402086\n",
      "ClassII:MITE:?                1089424\n",
      "ClassII:Maverick:?             411043\n",
      "ClassII:Maverick:Maverick       32983\n",
      "ClassII:TIR:?                 5176762\n",
      "ClassII:TIR:CACTA             2468455\n",
      "ClassII:TIR:MuDR              4269984\n",
      "ClassII:TIR:P                  273240\n",
      "ClassII:TIR:PIF-Harbinger     4470021\n",
      "ClassII:TIR:PiggyBac              345\n",
      "ClassII:TIR:Tc1-Mariner       3164294\n",
      "ClassII:TIR:Transib              1257\n",
      "ClassII:TIR:hAT               8656260\n",
      "SSR                           1923327\n",
      "noCat                         1176049\n",
      "Name: Length, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the summary of overlapping coverage according to Superfamily\")\n",
    "print(p_repet_gff_len_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the number of unique TEs: 12807\n",
      "This is the number of unique TE superfamilies: 42\n"
     ]
    }
   ],
   "source": [
    "num_unique_TEs = len(p_repet_gff_filtered[~p_repet_gff_filtered[1].str.contains('SSR')]['ID'].unique())\n",
    "num_unique_TE_super = len(p_repet_gff_filtered[~p_repet_gff_filtered[1].str.contains('SSR')]['Class:Order:Superfamily'].unique())\n",
    "\n",
    "print('This is the number of unique TEs: %i\\nThis is the number of unique TE superfamilies: %i' % (num_unique_TEs, num_unique_TE_super))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check out some pivot tables for the TE summarize in the python book\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class            Order     Superfamily  \n",
       "DNA_transposon   Crypton   Crypton             11\n",
       "                 Helitron  Helitron          3199\n",
       "                           noCat              992\n",
       "                 MITE      noCat             4084\n",
       "                 Maverick  Maverick            42\n",
       "                           noCat              281\n",
       "                 TIR       CACTA             3584\n",
       "                           MuDR              8939\n",
       "                           P                  957\n",
       "                           PIF-Harbinger    10660\n",
       "                           PiggyBac             3\n",
       "                           Tc1-Mariner       8122\n",
       "                           Transib              7\n",
       "                           hAT              20443\n",
       "                           noCat             7366\n",
       "                 noCat     Academ            2767\n",
       "                           Ginger1             53\n",
       "                           Ginger2_TDD          1\n",
       "                           Kolobok              3\n",
       "                           Sola                20\n",
       "                           noCat            26106\n",
       "Retrotransposon  DIRS      DIRS              2875\n",
       "                           noCat                8\n",
       "                 LARD      noCat             5628\n",
       "                 LINE      I                 1889\n",
       "                           Jockey             239\n",
       "                           L1                  28\n",
       "                           R2                  11\n",
       "                           RTE                 36\n",
       "                           noCat               13\n",
       "                 LTR       Bel-Pao             25\n",
       "                           Copia            34361\n",
       "                           ERV               1547\n",
       "                           Gypsy            76311\n",
       "                           Retrovirus          88\n",
       "                           noCat             6974\n",
       "                 PLE       Penelope             6\n",
       "                 Penelope  noCat              114\n",
       "                 SINE      noCat              171\n",
       "                 TRIM      noCat              418\n",
       "                 noCat     noCat              261\n",
       "SSR              SSR       SSR              72926\n",
       "noCat            noCat     noCat             1489\n",
       "Name: Length, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_repet_gff_filtered.groupby(by=['Class','Order','Superfamily'])['Length'].count() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class:Order:Superfamily\n",
       "ClassI:?:?                     261\n",
       "ClassI:DIRS:?                    8\n",
       "ClassI:DIRS:DIRS              2875\n",
       "ClassI:LARD:?                 5628\n",
       "ClassI:LINE:?                   13\n",
       "ClassI:LINE:I                 1889\n",
       "ClassI:LINE:Jockey             239\n",
       "ClassI:LINE:L1                  28\n",
       "ClassI:LINE:R2                  11\n",
       "ClassI:LINE:RTE                 36\n",
       "ClassI:LTR:?                  6974\n",
       "ClassI:LTR:Bel-Pao              25\n",
       "ClassI:LTR:Copia             34361\n",
       "ClassI:LTR:ERV                1547\n",
       "ClassI:LTR:Gypsy             76311\n",
       "ClassI:LTR:Retrovirus           88\n",
       "ClassI:PLE:Penelope              6\n",
       "ClassI:Penelope:?              114\n",
       "ClassI:SINE:?                  171\n",
       "ClassI:TRIM:?                  418\n",
       "ClassII:?:?                  26106\n",
       "ClassII:?:Academ              2767\n",
       "ClassII:?:Ginger1               53\n",
       "ClassII:?:Ginger2_TDD            1\n",
       "ClassII:?:Kolobok                3\n",
       "ClassII:?:Sola                  20\n",
       "ClassII:Crypton:Crypton         11\n",
       "ClassII:Helitron:?             992\n",
       "ClassII:Helitron:Helitron     3199\n",
       "ClassII:MITE:?                4084\n",
       "ClassII:Maverick:?             281\n",
       "ClassII:Maverick:Maverick       42\n",
       "ClassII:TIR:?                 7366\n",
       "ClassII:TIR:CACTA             3584\n",
       "ClassII:TIR:MuDR              8939\n",
       "ClassII:TIR:P                  957\n",
       "ClassII:TIR:PIF-Harbinger    10660\n",
       "ClassII:TIR:PiggyBac             3\n",
       "ClassII:TIR:Tc1-Mariner       8122\n",
       "ClassII:TIR:Transib              7\n",
       "ClassII:TIR:hAT              20443\n",
       "SSR                          72926\n",
       "noCat                         1489\n",
       "Name: Length, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_repet_gff_filtered.groupby(by=['Class:Order:Superfamily'])['Length'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered.to_csv(out_dir+'/'+genome+'.REPET.long_v2.df', sep='\\t', header = None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make new gff files where the ID column is the superfamily level\n",
    "p_repet_gff_superfamily = p_repet_gff_filtered.iloc[:,:]\n",
    "p_repet_gff_superfamily[8] = p_repet_gff_superfamily['Class:Order:Superfamily']\n",
    "p_repet_gff_superfamily.iloc[:,0:9].to_csv(out_dir+'/'+genome+'.REPET.superfamily.gff', sep='\\t', header = None, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make new gff file where the ID column is the TE level\n",
    "p_repet_gff_TE = p_repet_gff_filtered.iloc[:,:]\n",
    "p_repet_gff_TE[8] = p_repet_gff_TE['ID']\n",
    "p_repet_gff_TE.iloc[:,0:9].to_csv(out_dir+'/'+genome+'.REPET.TE.gff', sep='\\t', header = None, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the directory structure to safe specific coverage files\n",
    "os.chdir(out_dir)\n",
    "TE_types = ['Retrotransposon', 'DNA_transposon', 'noCat', 'SSR']\n",
    "TE_path = [os.path.join(out_dir, x) for x in TE_types]\n",
    "TE_path_dict = dict(zip(TE_types, TE_path))\n",
    "for TE_type in TE_types:\n",
    "    new_path = os.path.join(out_dir, TE_type)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.mkdir(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id(_id, bed_object, repet_prefix):\n",
    "    #ClassI are retrotransposon form blast\n",
    "    if 'ClassI:' in _id:\n",
    "        out_path = TE_path_dict['Retrotransposon']   \n",
    "    #ClassII are DNA_transponson\n",
    "    elif 'ClassII' in _id:\n",
    "        out_path = TE_path_dict['DNA_transposon'] \n",
    "    #The rest with '_' should be REPET_TEs\n",
    "    elif _id == 'noCat':\n",
    "        out_path = TE_path_dict['noCat']\n",
    "    #everything without '_' at the end should be SSR\n",
    "    elif _id == 'SSR':\n",
    "        out_path = TE_path_dict['SSR']\n",
    "    out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "    result = bed_object.filter(id_filter, _id).saveas(out_fn)\n",
    "    cov_fn = out_fn.replace('gff','cov')\n",
    "    cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "    cov.saveas(cov_fn)\n",
    "    #_len = len(pd.read_csv(cov_fn, header=None, sep='\\t'))\n",
    "    #_dict[_id] = _len\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we create a function to pass only features for a particular\n",
    "# featuretype.  This is similar to a \"grep\" operation when applied to every\n",
    "# feature in a BedTool\n",
    "def id_filter(feature, _id):\n",
    "    if feature[8] == _id:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repet_prefix_TE = genome+'.REPET.TE'\n",
    "repet_prefix_S = genome+'.REPET.superfamily'\n",
    "p_genome_file = genome+'.genome_file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull in the classification gff, make classification array, loop over array to save all the cov_dataframes\n",
    "RE_TE_gff = pybedtools.BedTool(out_dir+'/'+genome+'.REPET.TE.gff')\n",
    "g_TE = RE_TE_gff.remove_invalid().saveas(out_dir+'/'+genome+'.REPET.TE.bedobject')\n",
    "#use the blast filtered dataframe as well\n",
    "RE_S_gff = pybedtools.BedTool(out_dir+'/'+genome+'.REPET.superfamily.gff')\n",
    "g_S = RE_S_gff.remove_invalid().saveas(out_dir+'/'+genome+'.REPET.superfamily.bedobject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "jobs = []\n",
    "bed_file = g_S\n",
    "superfamilies = p_repet_gff_superfamily[8].unique()\n",
    "for superfamily in superfamilies:\n",
    "    subset_id(superfamily, bed_file, repet_prefix_S)\n",
    "    print('Doing %s' % superfamily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_dir = os.path.abspath(os.path.curdir)\n",
    "#this caputures all REPET classifications add the superfamily level\n",
    "class_cov_files = []\n",
    "for dirpath, dirname, filenames in os.walk(cur_dir, topdown=True):\n",
    "    if dirpath == cur_dir:\n",
    "        continue\n",
    "    cov_files = [dirpath +'/'+x for x in os.listdir(dirpath) if x.endswith('.cov') and repet_prefix_S in x]\n",
    "    for file in cov_files:\n",
    "        class_cov_files.append(file)\n",
    "\n",
    "#make a large summary dataframe from all the cov files where the last \n",
    "df_list =[]\n",
    "class_cov_files.sort()\n",
    "for file in class_cov_files:\n",
    "    print(file)\n",
    "    tmp_df = pd.read_csv(file, sep='\\t', header = None)\n",
    "    tmp_df[\"Class:Order:Superfamily\"] = file.split('.')[-2]\n",
    "    tmp_df.drop_duplicates(inplace=True)\n",
    "    df_list.append(tmp_df)\n",
    "    print(file.split('.')[-2])\n",
    "\n",
    "df_REPET_classification = pd.concat(df_list)\n",
    "df_REPET_classification.to_csv(out_dir+'/'+ repet_prefix_S +'.cov', sep='\\t', header =None, index=None)\n",
    "\n",
    "cov_per_superfamily = df_REPET_classification.pivot_table(values=1, columns= \"Class:Order:Superfamily\", aggfunc='count')\n",
    "cov_per_contig_per_superfamily = df_REPET_classification.groupby([0, \"Class:Order:Superfamily\"])[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov_all_TEs = df_REPET_classification.drop_duplicates([0,1])\n",
    "cov_all_TEs = len(cov_all_TEs)\n",
    "\n",
    "cov_per_superfamily_df = cov_per_superfamily.append(pd.DataFrame.from_dict({'cov_all_TEs': cov_all_TEs}, orient='index'))\n",
    "\n",
    "cov_per_superfamily_df.rename(columns={0: 'bp'}, inplace=True)\n",
    "\n",
    "cov_per_superfamily_df['%'] = round(cov_per_superfamily_df['bp']/genome_size*100, 2)\n",
    "\n",
    "cov_per_superfamily_df.to_csv(out_dir+'/'+genome+'.REPET.summary.tab', sep='\\t')\n",
    "\n",
    "cov_per_superfamily_df['%'].plot.barh()\n",
    "plt.xlabel('% genome coverage')\n",
    "plt.ylabel('Class:Order:Superfamily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genome_df = pd.read_csv(p_genome_file, sep='\\t', header=None,names=['contig', 'length'])\n",
    "\n",
    "genome_size = genome_df['length'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov_per_superfamily_df/genome_size\n",
    "\n",
    "cov_per_class_noblast/genome_size\n",
    "\n",
    "(cov_per_class/genome_size*100).sum()\n",
    "\n",
    "(cov_per_class_noblast/genome_size*100).sum()\n",
    "\n",
    "len(df_REPET_classification)\n",
    "\n",
    "len(df_REPET_noblast_classification)\n",
    "\n",
    "len(df_REPET_noblast_classification.drop_duplicates([0,1]))/genome_size\n",
    "\n",
    "len(df_REPET_classification.drop_duplicates([0,1]))/genome_size\n",
    "\n",
    "df_REPET_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83355616"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "jobs = []\n",
    "bed_file = RE_S_gff\n",
    "superfamilies = p_repet_gff_superfamily[8].unique()\n",
    "for superfamily in superfamilies:\n",
    "    p = multiprocessing.Process(target=subset_id, args=(superfamily, bed_file, repet_prefix_S,))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(60)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this caputures all REPET classifications including blast and internal REPET_TEs\n",
    "#the problem found here is that the blast and the REPET annotation is sometimes contratictory and overlapping\n",
    "class_cov_files = []\n",
    "for dirpath, dirname, filenames in os.walk(cur_dir, topdown=True):\n",
    "    if dirpath == cur_dir:\n",
    "        continue\n",
    "    cov_files = [dirpath +'/'+x for x in os.listdir(dirpath) if x.endswith('.cov') and 'REPET.classification' in x]\n",
    "    for file in cov_files:\n",
    "        class_cov_files.append(file)\n",
    "\n",
    "#make a large summary dataframe from all the cov files where the last \n",
    "df_list =[]\n",
    "class_cov_files.sort()\n",
    "for file in class_cov_files:\n",
    "    tmp_df = pd.read_csv(file, sep='\\t', header = None)\n",
    "    tmp_df[\"Code long\"] = file.split('.')[-2]\n",
    "    tmp_df.drop_duplicates(inplace=True)\n",
    "    df_list.append(tmp_df)\n",
    "    print(file.split('.')[-2])\n",
    "\n",
    "df_REPET_classification = pd.concat(df_list)\n",
    "df_REPET_classification.to_csv(out_dir+'/'+genome+'.REPET.classification.all.cov', sep='\\t', header =None, index=None)\n",
    "\n",
    "cov_per_class = df_REPET_classification.pivot_table(values=1, columns= 'Code long', aggfunc='count')\n",
    "cov_per_contig_per_class = df_REPET_classification.groupby([0, 'Code long'])[1].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_repet_gff_superfamily[8].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   ClassII:TIR:hAT\n",
       "2                 ClassII:TIR:CACTA\n",
       "4                 ClassII:TIR:CACTA\n",
       "6                 ClassII:TIR:CACTA\n",
       "7                 ClassII:TIR:CACTA\n",
       "8                 ClassII:TIR:CACTA\n",
       "9                 ClassII:TIR:CACTA\n",
       "11                ClassII:TIR:CACTA\n",
       "13                ClassII:TIR:CACTA\n",
       "15                ClassII:TIR:CACTA\n",
       "17                ClassII:TIR:CACTA\n",
       "19                ClassII:TIR:CACTA\n",
       "21                ClassII:TIR:CACTA\n",
       "23                ClassII:TIR:CACTA\n",
       "25                ClassII:TIR:CACTA\n",
       "27                ClassII:TIR:CACTA\n",
       "29                ClassII:TIR:CACTA\n",
       "31                ClassII:TIR:CACTA\n",
       "33                ClassII:TIR:CACTA\n",
       "35                ClassII:TIR:CACTA\n",
       "37                ClassII:TIR:CACTA\n",
       "39                ClassII:TIR:CACTA\n",
       "41                ClassII:TIR:CACTA\n",
       "43                ClassII:TIR:CACTA\n",
       "45                ClassII:TIR:CACTA\n",
       "47                ClassII:TIR:CACTA\n",
       "49                 ClassI:LTR:Gypsy\n",
       "51                 ClassI:LTR:Gypsy\n",
       "53        ClassII:TIR:PIF-Harbinger\n",
       "55        ClassII:TIR:PIF-Harbinger\n",
       "                    ...            \n",
       "539635                ClassII:TIR:?\n",
       "539637              ClassII:TIR:hAT\n",
       "539639                  ClassII:?:?\n",
       "539641                  ClassII:?:?\n",
       "539643                        noCat\n",
       "539645             ClassI:LTR:Gypsy\n",
       "539647             ClassI:LTR:Copia\n",
       "539649               ClassII:MITE:?\n",
       "539651               ClassII:MITE:?\n",
       "539653                  ClassII:?:?\n",
       "539655                 ClassI:LTR:?\n",
       "539657             ClassII:TIR:MuDR\n",
       "539659                ClassII:TIR:?\n",
       "539660                ClassII:TIR:?\n",
       "539661                ClassII:TIR:?\n",
       "539662             ClassII:TIR:MuDR\n",
       "539666                ClassII:TIR:?\n",
       "539668               ClassII:MITE:?\n",
       "539670             ClassI:LTR:Copia\n",
       "539672            ClassII:TIR:CACTA\n",
       "539674                ClassII:TIR:?\n",
       "539676               ClassII:MITE:?\n",
       "539678                          SSR\n",
       "539680             ClassI:LTR:Copia\n",
       "539681             ClassI:LTR:Copia\n",
       "539682             ClassI:LTR:Copia\n",
       "539683             ClassII:TIR:MuDR\n",
       "539685             ClassII:TIR:MuDR\n",
       "539687             ClassII:TIR:MuDR\n",
       "539689             ClassII:TIR:MuDR\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_repet_gff_superfamily[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_superfamily[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Next, we create a function to pass only features for a particular\n",
    "# featuretype.  This is similar to a \"grep\" operation when applied to every\n",
    "# feature in a BedTool\n",
    "def id_filter_col8(feature, _id):\n",
    "    if feature[8] == _id:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id_classification(_id, bed_object):\n",
    "    #retrotransposon \n",
    "    if 'Retrotransposon' in _id:\n",
    "        out_path = TE_path_dict['Retrotransposon']   \n",
    "    #DNA_transponson\n",
    "    elif 'DNA_transposon' in _id:\n",
    "        out_path = TE_path_dict['DNA_transposon'] \n",
    "    #notCat \n",
    "    elif 'noCat' in _id:\n",
    "        out_path = TE_path_dict['noCat']\n",
    "    #SSR\n",
    "    elif 'SSR' in _id:\n",
    "        out_path = TE_path_dict['SSR']\n",
    "    repet_prefix = genome+ '.'+bed_object.fn.split('.')[-3] + '.classification'\n",
    "    out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "    out_fn= out_fn.replace(\" \", '_')\n",
    "    result = bed_object.filter(id_filter_classification, _id).saveas(out_fn)\n",
    "    cov_fn = out_fn.replace('gff','cov')\n",
    "    cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "    cov.saveas(cov_fn)\n",
    "    print(\"Done with %s using %s as bedfile \" % (out_fn.split('/')[-1], bed_object.fn.split('/')[-1]))\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pull in the classification gff, make classification array, loop over array to save all the cov_dataframes\n",
    "RE_id_gff = pybedtools.BedTool(out_dir+'/'+genome+'.REPET.classification.gff')\n",
    "g = RE_id_gff.remove_invalid().saveas(out_dir+'/'+genome+'.REPET.classification.bedobject')\n",
    "#use the blast filtered dataframe as well\n",
    "RE_id_gff_noblast = pybedtools.BedTool(out_dir+'/'+genome+'.REPET_noblast.classification.gff')\n",
    "g_noblast = RE_id_gff_noblast.remove_invalid().saveas(out_dir+'/'+genome+'.REPET_noblast.classification.bedobject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the blast filtered dataframe as well\n",
    "RE_id_gff_blast = pybedtools.BedTool(out_dir+'/'+genome+'.REPET_blast.classification.gff')\n",
    "g_blast = RE_id_gff_blast.remove_invalid().saveas(out_dir+'/'+genome+'.REPET_blast.classification.bedobject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter through the whole REPET_TE bedobject\n",
    "#maybe make some multiproccesses out of this\n",
    "classifications = p_repet_gff_filtered_2[\"Classification\"].unique()\n",
    "#[subset_id_classification(x, g) for x in classifications]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "jobs = []\n",
    "bed_file = g\n",
    "for classi in classifications:\n",
    "    p = multiprocessing.Process(target=subset_id_classification, args=(classi, bed_file,))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate a coverage file for each genome position from the gff without filtering for specific features\n",
    "all_cov_RE = g.genome_coverage(dz=True,g=p_genome_file)\n",
    "all_cov_RE.saveas('Pst_104E_v12_p_ctg.REPET.classification.cov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print out the exitcodes for each\n",
    "for j in jobs:\n",
    "    #j.join()\n",
    "    print('%s.exitcode = %s' % (j.name, j.exitcode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter through the whole REPET_TE bedobject having removed the blast hits\n",
    "#maybe make some multiproccesses out of this\n",
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "jobs = []\n",
    "classifications_noblast = p_repet_gff_filtered_2[~p_repet_gff_filtered_2[1].str.contains('blast')][\"Classification\"].unique()\n",
    "classifications = classifications_noblast\n",
    "bed_file = g_noblast\n",
    "for classi in classifications:\n",
    "    p = multiprocessing.Process(target=subset_id_classification, args=(classi, bed_file,))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done!\")\n",
    "#print out the exitcodes for each\n",
    "for j in jobs:\n",
    "    #j.join()\n",
    "    print('%s.exitcode = %s' % (j.name, j.exitcode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_dir = os.path.abspath(os.path.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genome_df = pd.read_csv(p_genome_file, sep='\\t', header=None,names=['contig', 'length'])\n",
    "\n",
    "genome_size = genome_df['length'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this caputures all REPET classifications including blast and internal REPET_TEs\n",
    "#the problem found here is that the blast and the REPET annotation is sometimes contratictory and overlapping\n",
    "class_cov_files = []\n",
    "for dirpath, dirname, filenames in os.walk(cur_dir, topdown=True):\n",
    "    if dirpath == cur_dir:\n",
    "        continue\n",
    "    cov_files = [dirpath +'/'+x for x in os.listdir(dirpath) if x.endswith('.cov') and 'REPET.classification' in x]\n",
    "    for file in cov_files:\n",
    "        class_cov_files.append(file)\n",
    "\n",
    "#make a large summary dataframe from all the cov files where the last \n",
    "df_list =[]\n",
    "class_cov_files.sort()\n",
    "for file in class_cov_files:\n",
    "    tmp_df = pd.read_csv(file, sep='\\t', header = None)\n",
    "    tmp_df[\"Code long\"] = file.split('.')[-2]\n",
    "    tmp_df.drop_duplicates(inplace=True)\n",
    "    df_list.append(tmp_df)\n",
    "    print(file.split('.')[-2])\n",
    "\n",
    "df_REPET_classification = pd.concat(df_list)\n",
    "df_REPET_classification.to_csv(out_dir+'/'+genome+'.REPET.classification.all.cov', sep='\\t', header =None, index=None)\n",
    "\n",
    "cov_per_class = df_REPET_classification.pivot_table(values=1, columns= 'Code long', aggfunc='count')\n",
    "cov_per_contig_per_class = df_REPET_classification.groupby([0, 'Code long'])[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this caputures all REPET classifications including internal REPET_TEs classification only. No blast hits included\n",
    "class_cov_files = []\n",
    "for dirpath, dirname, filenames in os.walk(cur_dir, topdown=True):\n",
    "    if dirpath == cur_dir:\n",
    "        continue\n",
    "    cov_files = [dirpath +'/'+x for x in os.listdir(dirpath) if x.endswith('.cov') and 'REPET_noblast.classification' in x]\n",
    "    for file in cov_files:\n",
    "        class_cov_files.append(file)\n",
    "\n",
    "#make a large summary dataframe from all the cov files where the last \n",
    "df_list =[]\n",
    "class_cov_files.sort()\n",
    "for file in class_cov_files:\n",
    "    tmp_df = pd.read_csv(file, sep='\\t', header = None)\n",
    "    tmp_df[\"Code long\"] = file.split('.')[-2]\n",
    "    tmp_df.drop_duplicates(inplace=True)\n",
    "    df_list.append(tmp_df)\n",
    "    print(file.split('.')[-2])\n",
    "\n",
    "df_REPET_noblast_classification = pd.concat(df_list)\n",
    "df_REPET_noblast_classification.to_csv(out_dir+'/'+genome+'.REPET_noblast.classification.all.cov', sep='\\t', header =None, index=None)\n",
    "cov_per_class_noblast = df_REPET_noblast_classification.pivot_table(values=1, columns= 'Code long', aggfunc='count')\n",
    "cov_per_contig_per_class_noblast = df_REPET_noblast_classification.groupby([0, 'Code long'])[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_REPET_classification.to_csv(out_dir+'/'+genome+'.REPET.classification.all.cov', sep='\\t', header =None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov_per_class/genome_size\n",
    "\n",
    "cov_per_class_noblast/genome_size\n",
    "\n",
    "(cov_per_class/genome_size*100).sum()\n",
    "\n",
    "(cov_per_class_noblast/genome_size*100).sum()\n",
    "\n",
    "len(df_REPET_classification)\n",
    "\n",
    "len(df_REPET_noblast_classification)\n",
    "\n",
    "len(df_REPET_noblast_classification.drop_duplicates([0,1]))/genome_size\n",
    "\n",
    "len(df_REPET_classification.drop_duplicates([0,1]))/genome_size\n",
    "\n",
    "df_REPET_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look at the same graph with blast only searches and see what we get. Consider that a different parsing of ideas might be\n",
    "#more appropriate in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below I am trying to sort out the cov files produced from bedfiles for each indiviudal TE element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we create a function to pass only features for a particular\n",
    "# featuretype.  This is similar to a \"grep\" operation when applied to every\n",
    "# feature in a BedTool\n",
    "def id_filter(feature, _id):\n",
    "    if feature[8] == _id:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id(_id):\n",
    "    repet_prefix = 'Pst_104E_v12_p_ctg.REPET.TE_level'\n",
    "    #ClassI are retrotransposon form blast\n",
    "    if 'ClassI:' in _id:\n",
    "        out_path = TE_path_dict['Retrotransposon']   \n",
    "    #ClassII are DNA_transponson\n",
    "    elif 'ClassII' in _id:\n",
    "        out_path = TE_path_dict['DNA_transposon'] \n",
    "    #The rest with '_' should be REPET_TEs\n",
    "    elif _id.split('_')[0] in list(code_dict.keys()):\n",
    "        key = code_dict[_id.split('_')[0]].split(' ')[0]\n",
    "        out_path = TE_path_dict[key]\n",
    "    #everything without '_' at the end should be SSR\n",
    "    elif '_' not in _id:\n",
    "        out_path = TE_path_dict['SSR']\n",
    "    out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "    result = REPET_ID_bed.filter(id_filter, _id).saveas(out_fn)\n",
    "    cov_fn = out_fn.replace('gff','cov')\n",
    "    cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "    cov.saveas(cov_fn)\n",
    "    #_len = len(pd.read_csv(cov_fn, header=None, sep='\\t'))\n",
    "    #_dict[_id] = _len\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a loop that takes in the filtered id'ed gff file, filters the file according to the id field. The corresponding bedfile is saved in a specific folder for the type of TE (RNA, DNA, SSR, noCat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_types = ['Retrotransposon', 'DNA_transposon', 'noCat', 'SSR']\n",
    "TE_path = [os.path.join(out_dir, x) for x in TE_types]\n",
    "TE_path_dict = dict(zip(TE_types, TE_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for TE_type in TE_types:\n",
    "    new_path = os.path.join(out_dir, TE_type)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.mkdir(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_id = pd.read_csv(out_dir+'/'+genome+'.REPET.ID_column.gff', header=None, sep='\\t')[8].unique()\n",
    "REPET_ID_bed = pybedtools.BedTool(out_dir+'/'+genome+'.REPET.ID_column.gff')\n",
    "REPET_ID_bed = REPET_ID_bed.remove_invalid().saveas(out_dir+'/'+genome+'.REPET.ID_column.bedobject')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "\n",
    "jobs = []\n",
    "bed_file = g\n",
    "total_jobs = len(_id)\n",
    "job_count = 0\n",
    "manager = multiprocessing.Manager()\n",
    "TE_cov_df  = manager.dict()\n",
    "while job_count < total_jobs:\n",
    "    while [j.is_alive() for j in jobs].count(True) > 50:\n",
    "        time.sleep(15)\n",
    "        print(\"Waiting for Subset_id_classification to finish! More than 10 jobs running\")\n",
    "        print('%s jobs started' % job_count)\n",
    "    if job_count +10 < total_jobs:\n",
    "        for i in range(job_count, job_count+10):\n",
    "            p = multiprocessing.Process(target=subset_id, args=([_id[i],TE_cov_df]))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            job_count += 1\n",
    "            \n",
    "    else:\n",
    "        for i in range(job_count, total_jobs+1):\n",
    "            p = multiprocessing.Process(target=subset_id, args=([_id[i], TE_cov_df]))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            job_count += 1\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done! Totaling %i\"% (job_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(job_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id_q(_id_list, out_q):\n",
    "    outdict ={}\n",
    "    repet_prefix = 'Pst_104E_v12_p_ctg.REPET.TE_level'\n",
    "    for _id in _id_list:\n",
    "    \n",
    "        #ClassI are retrotransposon form blast\n",
    "        if 'ClassI:' in _id:\n",
    "            out_path = TE_path_dict['Retrotransposon']   \n",
    "        #ClassII are DNA_transponson\n",
    "        elif 'ClassII' in _id:\n",
    "            out_path = TE_path_dict['DNA_transposon'] \n",
    "        #The rest with '_' should be REPET_TEs\n",
    "        elif _id.split('_')[0] in list(code_dict.keys()):\n",
    "            key = code_dict[_id.split('_')[0]].split(' ')[0]\n",
    "            out_path = TE_path_dict[key]\n",
    "        #everything without '_' at the end should be SSR\n",
    "        elif '_' not in _id:\n",
    "            out_path = TE_path_dict['SSR']\n",
    "        out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "        result = REPET_ID_bed.filter(id_filter, _id).saveas(out_fn)\n",
    "        cov_fn = out_fn.replace('gff','cov')\n",
    "        cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "        cov.saveas(cov_fn)\n",
    "        _len = len(pd.read_csv(cov_fn, header=None, sep='\\t'))\n",
    "        outdict[_id] = _len\n",
    "    out_q.put(outdict)\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mp_factorizer(nums, nprocs):\n",
    "    def worker(nums, out_q):\n",
    "        \"\"\" The worker function, invoked in a process. 'nums' is a\n",
    "            list of numbers to factor. The results are placed in\n",
    "            a dictionary that's pushed to a queue.\n",
    "        \"\"\"\n",
    "        outdict = {}\n",
    "        for n in nums:\n",
    "            outdict[n] = factorize_naive(n)\n",
    "        out_q.put(outdict)\n",
    "\n",
    "    # Each process will get 'chunksize' nums and a queue to put his out\n",
    "    # dict into\n",
    "    out_q = Queue()\n",
    "    chunksize = int(math.ceil(len(nums) / float(nprocs)))\n",
    "    procs = []\n",
    "\n",
    "    for i in range(nprocs):\n",
    "        p = multiprocessing.Process(\n",
    "                target=worker,\n",
    "                args=(nums[chunksize * i:chunksize * (i + 1)],\n",
    "                      out_q))\n",
    "        procs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    # Collect all results into a single result dict. We know how many dicts\n",
    "    # with results to expect.\n",
    "    resultdict = {}\n",
    "    for i in range(nprocs):\n",
    "        resultdict.update(out_q.get())\n",
    "\n",
    "    # Wait for all worker processes to finish\n",
    "    for p in procs:\n",
    "        p.join()\n",
    "\n",
    "    return resultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "import math\n",
    "out_q = multiprocessing.Queue()\n",
    "nproces = 30 #number of processors\n",
    "chunksize = int(math.ceil(len(_id) / float(nproces)))\n",
    "jobs = []\n",
    "job_count = 0\n",
    "#manager = multiprocessing.Manager()\n",
    "#TE_cov_df  = manager.dict()\n",
    "for i in range(nproces):\n",
    "        p = multiprocessing.Process(target=subset_id_q, args=(_id[chunksize * i:chunksize * (i + 1)],out_q))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        job_count += 1\n",
    "# Collect all results into a single result dict. We know how many dicts\n",
    "# with results to expect.\n",
    "resultdict = {}\n",
    "for i in range(nproces):\n",
    "    resultdict.update(out_q.get())\n",
    "\n",
    "# Wait for all worker processes to finish\n",
    "for p in jobs:\n",
    "    p.join()\n",
    "\n",
    "        \n",
    "        \n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done! Totaling %i\"% (job_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = multiprocessing.Pool(20)\n",
    "p.map(subset_id, _id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id_m(_id,):\n",
    "    repet_prefix = 'Pst_104E_v12_p_ctg.REPET.TE_level'\n",
    "    #ClassI are retrotransposon form blast\n",
    "    if 'ClassI:' in _id:\n",
    "        out_path = TE_path_dict['Retrotransposon']   \n",
    "    #ClassII are DNA_transponson\n",
    "    elif 'ClassII' in _id:\n",
    "        out_path = TE_path_dict['DNA_transposon'] \n",
    "    #The rest with '_' should be REPET_TEs\n",
    "    elif _id.split('_')[0] in list(code_dict.keys()):\n",
    "        key = code_dict[_id.split('_')[0]].split(' ')[0]\n",
    "        out_path = TE_path_dict[key]\n",
    "    #everything without '_' at the end should be SSR\n",
    "    elif '_' not in _id:\n",
    "        out_path = TE_path_dict['SSR']\n",
    "    out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "    result = REPET_ID_bed.filter(id_filter, _id).saveas(out_fn)\n",
    "    cov_fn = out_fn.replace('gff','cov')\n",
    "    cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "    cov.saveas(cov_fn)\n",
    "    #_len = len(pd.read_csv(cov_fn, header=None, sep='\\t'))\n",
    "    #_dict[_id] = _len\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "jobs = []\n",
    "bed_file = g\n",
    "total_jobs = len(_id)\n",
    "job_count = 0\n",
    "manager = multiprocessing.Manager()\n",
    "TE_cov_df  = manager.dict()\n",
    "\n",
    "    while [j.is_alive() for j in jobs].count(True) > 10:\n",
    "        time.sleep(15)\n",
    "        print(\"Waiting for Subset_id_classification to finish! More than 10 jobs running\")\n",
    "        print('%s jobs started' % job_count)\n",
    "    if job_count +10 < total_jobs:\n",
    "        for i in range(job_count, job_count+10):\n",
    "            p = multiprocessing.Process(target=subset_id, args=([_id[i],TE_cov_df]))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            job_count += 1\n",
    "            \n",
    "    else:\n",
    "        for i in range(job_count, total_jobs+1):\n",
    "            p = multiprocessing.Process(target=subset_id, args=([_id[i], TE_cov_df]))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            job_count += 1\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done! Totaling %i\"% (job_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in TE_cov_df.keys():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[j.is_alive() for j in jobs].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is REALLY slow for now. Would need to parallize this step. Look at the pool function of \n",
    "#multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[subset_id(x) for x in _id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this caputures all REPET cov files for each indiviual TE.\n",
    "class_cov_files = []\n",
    "for dirpath, dirname, filenames in os.walk(cur_dir):\n",
    "    #print(dirpath)\n",
    "    #print(len(filenames))\n",
    "    if len(filenames) == 0:  # empty folder\n",
    "        continue\n",
    "    cov_files = [dirpath +'/'+x for x in filenames if x.endswith('.cov') and 'classification' not in x ]\n",
    "    for file in cov_files:\n",
    "        class_cov_files.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cov_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#make a large summary dataframe from all the cov files where the last \n",
    "df_list =[]\n",
    "class_cov_files.sort()\n",
    "for file in class_cov_files:\n",
    "    tmp_df = pd.read_csv(file, sep='\\t', header = None)\n",
    "    tmp_df[\"Code long\"] = file.split('.')[-2]\n",
    "    tmp_df.drop_duplicates(inplace=True)\n",
    "    df_list.append(tmp_df)\n",
    "    print(file.split('.')[-2])\n",
    "\n",
    "df_REPET_noblast_classification = pd.concat(df_list)\n",
    "\n",
    "cov_per_class_noblast = df_REPET_noblast_classification.pivot_table(values=1, columns= 'Code long', aggfunc='count')\n",
    "cov_per_contig_per_class_noblast = df_REPET_noblast_classification.groupby([0, 'Code long'])[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
