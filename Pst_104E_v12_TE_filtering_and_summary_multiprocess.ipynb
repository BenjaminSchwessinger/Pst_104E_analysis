{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/Bio/SearchIO/__init__.py:211: BiopythonExperimentalWarning: Bio.SearchIO is an experimental submodule which may undergo significant changes prior to its future official release.\n",
      "  BiopythonExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "import pysam\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SearchIO\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pybedtools\n",
    "import multiprocessing\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_dir = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genome = 'Pst_104E_v12_p_ctg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dir = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12/TE_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove all commenting lines from the initial repet file\n",
    "!grep -v \"^#\" {source_dir}/{genome}.REPET.gff > {out_dir}/{genome}.REPET.gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff = pd.read_csv(out_dir+'/'+genome+'.REPET.gff', sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TE_post_analysis_p = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/REPET/Pst79_p/Pst79_p_full_annotate/postanalysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_post_analysis_p_header = 'TE      length  covg    frags   fullLgthFrags   copies  fullLgthCopies  meanId  sdId    minId   q25Id   medId   q75Id   maxId   meanLgth        sdLgth  minLgth q25Lgth medLgth q75Lgth maxLgth meanLgthPerc    sdLgthPerc      minLgthPerc  q25LgthPerc     medLgthPerc     q75LgthPerc     maxLgthPerc'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TE_post_analysis_p_header = [x for x in TE_post_analysis_p_header if x != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this needs to be fixed up to pick the proper summary table\n",
    "p_repet_summary_df = pd.read_csv(TE_post_analysis_p+'/'+'Pst79p_anno_chr_allTEs_nr_noSSR_join_path.annotStatsPerTE.tab' ,\\\n",
    "                                names = TE_post_analysis_p_header, header=None, sep='\\t', skiprows=1 )\n",
    "\n",
    "#check if I can filter the tab files for removing all TEs that are on the 2000 plus contigs\n",
    "#remove tRNAs TEs with infernal\n",
    "\n",
    "p_repet_summary_df['Code'] = p_repet_summary_df['TE'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "code_keys = p_repet_summary_df['Code'].unique()\n",
    "\n",
    "code_keys.sort()\n",
    "\n",
    "code_long = ['DNA_transposon Helitron', 'DNA_transposon Helitron', 'DNA_transposon Helitron', 'DNA_transposon Maverick',\\\n",
    "            'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon TIR', 'DNA_transposon noCat',\\\n",
    "             'DNA_transposon MITE','DNA_transposon MITE', 'Potential Host Gene', 'Retrotransposon LINE', 'Retrotransposon LINE',\\\n",
    "             'Retrotransposon LINE','Retrotransposon LTR','Retrotransposon LTR', 'Retrotransposon LTR', 'Retrotransposon LTR', 'Retrotransposon PLE', \\\n",
    "             'Retrotransposon SINE',  'Retrotransposon SINE', 'Retrotransposon noCat', 'Retrotransposon LARD',\\\n",
    "             'Retrotransposon LARD', 'Retrotransposon TRIM', 'Retrotransposon TRIM', 'Retrotransposon noCat',  \\\n",
    "             'Retrotransposon DIRS','Retrotransposon DIRS','Retrotransposon DIRS','Retrotransposon DIRS',\\\n",
    "             'noCat', 'noCat']\n",
    "\n",
    "code_dict = dict(zip(code_keys, code_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "three_letter_code = list({x[0:3] for x in code_keys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "three_letter_code.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "three_letter_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_letter_values = []\n",
    "for x in three_letter_code:\n",
    "    _value =''\n",
    "    if x[0] == 'D':\n",
    "        _value = _value + 'ClassII:'\n",
    "    if x[0] == 'R':\n",
    "        _value = _value + 'ClassI:'\n",
    "    if x[0] != 'D' and x[0] != 'R':\n",
    "        _value = 'noCat'\n",
    "        three_letter_values.append(_value)\n",
    "        continue\n",
    "    if x[1] == 'T':\n",
    "        _value = _value + 'TIR:?'\n",
    "    if x[1] == 'H':\n",
    "        _value = _value + 'Helitron:?'\n",
    "    if x[1] == 'M':\n",
    "        _value = _value + 'Maverick:?'\n",
    "    if x[0:2] == 'DY':\n",
    "        _value = _value + ':Crypton:?'\n",
    "    if x[1] == 'X':\n",
    "        _value = _value + '?:?'\n",
    "    if x[1] == 'I':\n",
    "        _value = _value + 'LINE:?'\n",
    "    if x[1] == 'L':\n",
    "        _value = _value + 'LTR:?'\n",
    "    if x[1] == 'P':\n",
    "        _value = _value + 'Penelope:?'\n",
    "    if x[1] == 'S':\n",
    "        _value = _value + 'SINE:?'\n",
    "    if x[0:2] == 'RY':\n",
    "        _value = _value + 'DIRS:?'\n",
    "    \n",
    "        \n",
    "    three_letter_values.append(_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(three_letter_code) == len(three_letter_values):\n",
    "    print(\"Aas\")\n",
    "    three_letter_dict = dict(zip(three_letter_code, three_letter_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "three_letter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_summary_df['Code long'] = p_repet_summary_df['Code'].apply(lambda x: code_dict[x])\n",
    "\n",
    "p_repet_summary_sum_df = pd.pivot_table(p_repet_summary_df, values=['covg', 'copies'], index='Code long', aggfunc=np.sum)\n",
    "\n",
    "p_repet_summary_mean_df = pd.pivot_table(p_repet_summary_df, values='length', index='Code long', aggfunc=np.mean)\n",
    "\n",
    "pd.concat([p_repet_summary_sum_df,p_repet_summary_mean_df], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pull out the top 20 TEs based on coverage in the coverage dataframe\n",
    "p_repet_top_20 = p_repet_summary_df[~(p_repet_summary_df['Code']=='PotentialHostGene')].sort_values(by='covg', ascending=False).reset_index(drop=True).iloc[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_top_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_top_20_TE = p_repet_top_20['TE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "['_'.join(x.split('_')[:2]) for x in p_repet_top_20_TE]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p_repet_gff needs to be filtered to remove all the the high coverage contigs and such\n",
    "the filters are located in this folder\n",
    "/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now filter the gff dataframe to delete all the high coverage contigs\n",
    "#This might would have to be fixed as well. If we don't delete it as files should be already filtered\n",
    "contigs_smaller_2000 = pd.read_csv('/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly/pcontig_smaller_2000.txt',\\\n",
    "                                  header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_gff = pd.read_csv(out_dir+'/'+genome+'.REPET.gff', sep='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered = p_repet_gff[p_repet_gff[0].isin(contigs_smaller_2000)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ID_filter_gff(_feature, _id):\n",
    "    \"\"\"\n",
    "    This filter parses out the top level id form the 9th gff column form a REPET gff file.\n",
    "    It has a specific search pattern for each feature type in column 2.\n",
    "    _type is defined by the feature '_'.join(feature.split(\"_\")[-2:])\n",
    "    \"\"\"\n",
    "    _type = '_'.join(_feature.split(\"_\")[-2:])\n",
    "    if _type == 'REPET_TEs':\n",
    "\n",
    "        TE_pattern = r'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([^;| ]*)'\n",
    "        TE_prog = re.compile(TE_pattern)\n",
    "        TE_match = TE_prog.search(_id)\n",
    "\n",
    "        try:\n",
    "            return TE_match.group(1)\n",
    "        except AttributeError:\n",
    "            print(_id)\n",
    "\n",
    "    if _type == 'REPET_SSRs':\n",
    "        \n",
    "        SSR_pattern = 'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([A-Z,a-z,0-9,-]*)'\n",
    "        SSR_prog = re.compile(SSR_pattern)\n",
    "        SSR_match = SSR_prog.search(_id)\n",
    "        return SSR_match.group(1)\n",
    "    if _type == 'REPET_tblastx' or _type == 'REPET_blastx':\n",
    "        #if \"#\" in _id:\n",
    "        #     blast_pattern = 'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([A-Z,a-z,0-9,-]*[_]?[A-Z,a-z,0-9,-]*[_|#|0-9]+?:[A-Z,a-z,0-9,-,:]*)'\n",
    "        #else:\n",
    "        #    blast_pattern = r'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([\\w+|:|-]*)'\n",
    "        blast_prog = re.compile(r'ID=[A-Z,a-z,0-9,-]*_[A-Z,a-z,0-9]*_[0-9]*_([^;| ]*)')\n",
    "        #blast_prog = re.compile(blast_pattern)\n",
    "        blast_match = blast_prog.search(_id)\n",
    "        return blast_match.group(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def blast_hit_gff(_feature, _id, blast_id):\n",
    "    \"\"\"\n",
    "    This filter parses the blast hit for REPET_TEs from the new 'ID' column. If no blast hit available return N/A.\n",
    "    If the result is blast already the value is simple parse the blast hit.\n",
    "    SSRs also get N/A\n",
    "    _type is defined by the feature '_'.join(feature.split(\"_\")[-2:])\n",
    "    \"\"\"\n",
    "    _type = '_'.join(_feature.split(\"_\")[-2:])\n",
    "    if _type == 'REPET_TEs':\n",
    "        #split the pastec_cat into the first three letter code\n",
    "        pastec_cat = blast_id[0:3]\n",
    "        if 'TE_BLR' in _id:\n",
    "            #hit_list = [x.split(';')[3] for x in _id]\n",
    "            blast_hit_pattern = r'TE_BLR\\w*: (\\S*)[ |;]'\n",
    "            blast_hit_prog = re.compile(blast_hit_pattern)\n",
    "            TE_match = blast_hit_prog.findall(_id)\n",
    "            first_sub_class = ':'.join(TE_match[0][:-1].split(':')[1:])\n",
    "            if len([x for x in TE_match if first_sub_class in x]) == len(TE_match):\n",
    "                if ';' in first_sub_class:\n",
    "                    return first_sub_class.split(';')[0]\n",
    "                else:\n",
    "                    return first_sub_class\n",
    "#fix this here to include the there letter code of the first bit of the ID similar to the blast hits\n",
    "#e.g. ClassI:?:? and so on. a dict might be the easiest here.\n",
    "            \n",
    "            else:\n",
    "                return three_letter_dict[pastec_cat]\n",
    "        else:\n",
    "            return three_letter_dict[pastec_cat]\n",
    "    if _type == 'REPET_SSRs':\n",
    "        return 'noCat'\n",
    "        \n",
    "\n",
    "        return SSR_match.group(1)\n",
    "    if _type == 'REPET_tblastx' or _type == 'REPET_blastx':\n",
    "        return ':'.join(blast_id.split(':')[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered['ID'] = p_repet_gff_filtered.apply(lambda row: ID_filter_gff(row[1], row[8]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered['ID'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered['potential subclass'] = p_repet_gff_filtered.apply(lambda row: blast_hit_gff(row[1], row[8], row['ID']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter out potential host genes\n",
    "p_repet_gff_filtered = p_repet_gff_filtered[~p_repet_gff_filtered[8].str.contains(\"Potential\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered.iloc[3,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_2 = p_repet_gff_filtered.loc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_2.drop_duplicates(subset=[3,5,'ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_2['Length'] = p_repet_gff_2[4] - p_repet_gff_2[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_gff_len = p_repet_gff_2.groupby('potential subclass')['Length'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_len.sort_values(ascending=False)[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_unique_REs = len(p_repet_gff_filtered['ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('This is the number of unique repetitive elements: %i' % num_unique_REs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency_of_REs  = p_repet_gff_filtered.groupby('ID')[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency_of_REs.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered.to_csv(out_dir+'/'+genome+'.REPET.filtered.gff', sep='\\t', header = None, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered[8] = p_repet_gff_filtered['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered_2 = p_repet_gff_filtered.iloc[:,0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered_2.to_csv(out_dir+'/'+genome+'.REPET.ID_column.gff', sep='\\t', header = None, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#have another dataframe that only contains the REPET denovo annotation and not blast hits\n",
    "#repet_gff_filtered_TEs = p_repet_gff_filtered_2[~p_repet_gff_filtered_2[1].str.contains('blast')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get all the 'Code' phrases blast hits and make a blast_code_dict\n",
    "#this needs to get the subsetting filters set for different dataframes\n",
    "\n",
    "blast_codes = p_repet_gff_filtered_2[(p_repet_gff_filtered_2[1]=='Pst79p_anno_REPET_tblastx') | (p_repet_gff_filtered_2[1]=='Pst79p_anno_REPET_blastx')][8]\n",
    "\n",
    "blast_codes_list = [ ':'.join(x.split(':')[1:-1]) for x in blast_codes.unique()]\n",
    "\n",
    "blast_codes_list_unique = list(set(blast_codes_list))\n",
    "\n",
    "blast_codes_list_unique.sort()\n",
    "\n",
    "blast_codes_list_unique\n",
    "\n",
    "blast_code_long = ['Retrotransposon noCat', 'Retrotransposon DIRS', 'Retrotransposon LINE', 'Retrotransposon LTR', 'Retrotransposon PLE','DNA_transposon noCat',\\\n",
    "                   'DNA_transposon Crypton','DNA_transposon Helitron','DNA_transposon Maverick','DNA_transposon TIR']\n",
    "\n",
    "blast_code_dict = dict(zip(blast_codes_list_unique, blast_code_long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blast_codes_list_subclass = [ ':'.join(x.split(':')[1:]) for x in blast_codes.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a dataframe with only the blast hits as occurance and use this to generate a coverage plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blast_codes.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write a filer function that adds a 'code long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def code_long_filter_gff(_feature, _id):\n",
    "    \"\"\"\n",
    "    This filter parses out Order and class of the TE based on Wicker et al. using the previously generated ID column. \n",
    "    It has a specific search pattern for each feature type in column 2.\n",
    "    _type is defined by the feature '_'.join(feature.split(\"_\")[-2:])\n",
    "    \"\"\"\n",
    "    _type = '_'.join(_feature.split(\"_\")[-2:])\n",
    "    if _type == 'REPET_TEs':\n",
    "        #split the \n",
    "        code = _id.split('_')[0]\n",
    "        return code_dict[code]\n",
    "    if _type == 'REPET_SSRs':\n",
    "        return 'SSR'\n",
    "    if _type == 'REPET_tblastx' or _type == 'REPET_blastx':\n",
    "        code = ':'.join(_id.split(':')[1:-1])\n",
    "        return blast_code_dict[code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered_2['Classification'] = p_repet_gff_filtered_2.apply(lambda row: code_long_filter_gff(row[1], row[8]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered_2[\"Classification\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in the .classificaiton gff the feature column 2 is the Wicker classification of the transposon\n",
    "p_repet_gff_filtered_2[8] = p_repet_gff_filtered_2['Classification'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_repet_gff_filtered_2.iloc[:,:-1].to_csv(out_dir+'/'+genome+'.REPET.classification.gff', sep='\\t', header = None, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save the no_blast hit REPET gff as well.\n",
    "p_repet_gff_filtered_2[~p_repet_gff_filtered_2[1].str.contains('blast')].iloc[:,:-1].to_csv(out_dir+'/'+genome+'.REPET_noblast.classification.gff', sep='\\t', header = None, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the no_blast hit REPET gff as well.\n",
    "p_repet_gff_filtered_2[p_repet_gff_filtered_2[1].str.contains('blast')].iloc[:,:-1].to_csv(out_dir+'/'+genome+'.REPET_blast.classification.gff', sep='\\t', header = None, index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write a similar filter as before using bedtools to move through all the classifications and caclulate coverage\n",
    "#summarize this in a table and compare to published stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate the directory structure to safe specific coverage files\n",
    "os.chdir(out_dir)\n",
    "TE_types = ['Retrotransposon', 'DNA_transposon', 'noCat', 'SSR']\n",
    "TE_path = [os.path.join(out_dir, x) for x in TE_types]\n",
    "TE_path_dict = dict(zip(TE_types, TE_path))\n",
    "for TE_type in TE_types:\n",
    "    new_path = os.path.join(out_dir, TE_type)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.mkdir(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "repet_prefix = genome+'.REPET.classification'\n",
    "p_genome_file = genome+'.genome_file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we create a function to pass only features for a particular\n",
    "# featuretype.  This is similar to a \"grep\" operation when applied to every\n",
    "# feature in a BedTool\n",
    "def id_filter_classification(feature, _id):\n",
    "    if feature[8] == _id:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id_classification(_id, bed_object):\n",
    "    #retrotransposon \n",
    "    if 'Retrotransposon' in _id:\n",
    "        out_path = TE_path_dict['Retrotransposon']   \n",
    "    #DNA_transponson\n",
    "    elif 'DNA_transposon' in _id:\n",
    "        out_path = TE_path_dict['DNA_transposon'] \n",
    "    #notCat \n",
    "    elif 'noCat' in _id:\n",
    "        out_path = TE_path_dict['noCat']\n",
    "    #SSR\n",
    "    elif 'SSR' in _id:\n",
    "        out_path = TE_path_dict['SSR']\n",
    "    repet_prefix = genome+ '.'+bed_object.fn.split('.')[-3] + '.classification'\n",
    "    out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "    out_fn= out_fn.replace(\" \", '_')\n",
    "    result = bed_object.filter(id_filter_classification, _id).saveas(out_fn)\n",
    "    cov_fn = out_fn.replace('gff','cov')\n",
    "    cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "    cov.saveas(cov_fn)\n",
    "    print(\"Done with %s using %s as bedfile \" % (out_fn.split('/')[-1], bed_object.fn.split('/')[-1]))\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pull in the classification gff, make classification array, loop over array to save all the cov_dataframes\n",
    "RE_id_gff = pybedtools.BedTool(out_dir+'/'+genome+'.REPET.classification.gff')\n",
    "g = RE_id_gff.remove_invalid().saveas(out_dir+'/'+genome+'.REPET.classification.bedobject')\n",
    "#use the blast filtered dataframe as well\n",
    "RE_id_gff_noblast = pybedtools.BedTool(out_dir+'/'+genome+'.REPET_noblast.classification.gff')\n",
    "g_noblast = RE_id_gff_noblast.remove_invalid().saveas(out_dir+'/'+genome+'.REPET_noblast.classification.bedobject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the blast filtered dataframe as well\n",
    "RE_id_gff_blast = pybedtools.BedTool(out_dir+'/'+genome+'.REPET_blast.classification.gff')\n",
    "g_blast = RE_id_gff_blast.remove_invalid().saveas(out_dir+'/'+genome+'.REPET_blast.classification.bedobject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter through the whole REPET_TE bedobject\n",
    "#maybe make some multiproccesses out of this\n",
    "classifications = p_repet_gff_filtered_2[\"Classification\"].unique()\n",
    "#[subset_id_classification(x, g) for x in classifications]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "jobs = []\n",
    "bed_file = g\n",
    "for classi in classifications:\n",
    "    p = multiprocessing.Process(target=subset_id_classification, args=(classi, bed_file,))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate a coverage file for each genome position from the gff without filtering for specific features\n",
    "all_cov_RE = g.genome_coverage(dz=True,g=p_genome_file)\n",
    "all_cov_RE.saveas('Pst_104E_v12_p_ctg.REPET.classification.cov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print out the exitcodes for each\n",
    "for j in jobs:\n",
    "    #j.join()\n",
    "    print('%s.exitcode = %s' % (j.name, j.exitcode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filter through the whole REPET_TE bedobject having removed the blast hits\n",
    "#maybe make some multiproccesses out of this\n",
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "jobs = []\n",
    "classifications_noblast = p_repet_gff_filtered_2[~p_repet_gff_filtered_2[1].str.contains('blast')][\"Classification\"].unique()\n",
    "classifications = classifications_noblast\n",
    "bed_file = g_noblast\n",
    "for classi in classifications:\n",
    "    p = multiprocessing.Process(target=subset_id_classification, args=(classi, bed_file,))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done!\")\n",
    "#print out the exitcodes for each\n",
    "for j in jobs:\n",
    "    #j.join()\n",
    "    print('%s.exitcode = %s' % (j.name, j.exitcode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_dir = os.path.abspath(os.path.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genome_df = pd.read_csv(p_genome_file, sep='\\t', header=None,names=['contig', 'length'])\n",
    "\n",
    "genome_size = genome_df['length'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this caputures all REPET classifications including blast and internal REPET_TEs\n",
    "#the problem found here is that the blast and the REPET annotation is sometimes contratictory and overlapping\n",
    "class_cov_files = []\n",
    "for dirpath, dirname, filenames in os.walk(cur_dir, topdown=True):\n",
    "    if dirpath == cur_dir:\n",
    "        continue\n",
    "    cov_files = [dirpath +'/'+x for x in os.listdir(dirpath) if x.endswith('.cov') and 'REPET.classification' in x]\n",
    "    for file in cov_files:\n",
    "        class_cov_files.append(file)\n",
    "\n",
    "#make a large summary dataframe from all the cov files where the last \n",
    "df_list =[]\n",
    "class_cov_files.sort()\n",
    "for file in class_cov_files:\n",
    "    tmp_df = pd.read_csv(file, sep='\\t', header = None)\n",
    "    tmp_df[\"Code long\"] = file.split('.')[-2]\n",
    "    tmp_df.drop_duplicates(inplace=True)\n",
    "    df_list.append(tmp_df)\n",
    "    print(file.split('.')[-2])\n",
    "\n",
    "df_REPET_classification = pd.concat(df_list)\n",
    "df_REPET_classification.to_csv(out_dir+'/'+genome+'.REPET.classification.all.cov', sep='\\t', header =None, index=None)\n",
    "\n",
    "cov_per_class = df_REPET_classification.pivot_table(values=1, columns= 'Code long', aggfunc='count')\n",
    "cov_per_contig_per_class = df_REPET_classification.groupby([0, 'Code long'])[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this caputures all REPET classifications including internal REPET_TEs classification only. No blast hits included\n",
    "class_cov_files = []\n",
    "for dirpath, dirname, filenames in os.walk(cur_dir, topdown=True):\n",
    "    if dirpath == cur_dir:\n",
    "        continue\n",
    "    cov_files = [dirpath +'/'+x for x in os.listdir(dirpath) if x.endswith('.cov') and 'REPET_noblast.classification' in x]\n",
    "    for file in cov_files:\n",
    "        class_cov_files.append(file)\n",
    "\n",
    "#make a large summary dataframe from all the cov files where the last \n",
    "df_list =[]\n",
    "class_cov_files.sort()\n",
    "for file in class_cov_files:\n",
    "    tmp_df = pd.read_csv(file, sep='\\t', header = None)\n",
    "    tmp_df[\"Code long\"] = file.split('.')[-2]\n",
    "    tmp_df.drop_duplicates(inplace=True)\n",
    "    df_list.append(tmp_df)\n",
    "    print(file.split('.')[-2])\n",
    "\n",
    "df_REPET_noblast_classification = pd.concat(df_list)\n",
    "df_REPET_noblast_classification.to_csv(out_dir+'/'+genome+'.REPET_noblast.classification.all.cov', sep='\\t', header =None, index=None)\n",
    "cov_per_class_noblast = df_REPET_noblast_classification.pivot_table(values=1, columns= 'Code long', aggfunc='count')\n",
    "cov_per_contig_per_class_noblast = df_REPET_noblast_classification.groupby([0, 'Code long'])[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_REPET_classification.to_csv(out_dir+'/'+genome+'.REPET.classification.all.cov', sep='\\t', header =None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cov_all_TEs = df_REPET_classification.drop_duplicates([0,1])\n",
    "cov_all_TEs = len(cov_all_TEs)\n",
    "\n",
    "cov_per_class_df = cov_per_class.append(pd.DataFrame.from_dict({'cov_all_TEs': cov_all_TEs}, orient='index'))\n",
    "\n",
    "cov_per_class_df.rename(columns={0: 'bp'}, inplace=True)\n",
    "\n",
    "cov_per_class_df['%'] = round(cov_per_class_df['bp']/genome_size*100, 2)\n",
    "\n",
    "cov_per_class_df.to_csv(out_dir+'/'+genome+'.REPET.summary.tab', sep='\\t')\n",
    "\n",
    "cov_per_class_df['%'].plot.barh()\n",
    "plt.xlabel('% genome coverage')\n",
    "plt.ylabel('TE subclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cov_per_class/genome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cov_per_class_noblast/genome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(cov_per_class/genome_size*100).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(cov_per_class_noblast/genome_size*100).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_REPET_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_REPET_noblast_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_REPET_noblast_classification.drop_duplicates([0,1]))/genome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_REPET_classification.drop_duplicates([0,1]))/genome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_REPET_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look at the same graph with blast only searches and see what we get. Consider that a different parsing of ideas might be\n",
    "#more appropriate in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below I am trying to sort out the cov files produced from bedfiles for each indiviudal TE element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next, we create a function to pass only features for a particular\n",
    "# featuretype.  This is similar to a \"grep\" operation when applied to every\n",
    "# feature in a BedTool\n",
    "def id_filter(feature, _id):\n",
    "    if feature[8] == _id:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id(_id):\n",
    "    repet_prefix = 'Pst_104E_v12_p_ctg.REPET.TE_level'\n",
    "    #ClassI are retrotransposon form blast\n",
    "    if 'ClassI:' in _id:\n",
    "        out_path = TE_path_dict['Retrotransposon']   \n",
    "    #ClassII are DNA_transponson\n",
    "    elif 'ClassII' in _id:\n",
    "        out_path = TE_path_dict['DNA_transposon'] \n",
    "    #The rest with '_' should be REPET_TEs\n",
    "    elif _id.split('_')[0] in list(code_dict.keys()):\n",
    "        key = code_dict[_id.split('_')[0]].split(' ')[0]\n",
    "        out_path = TE_path_dict[key]\n",
    "    #everything without '_' at the end should be SSR\n",
    "    elif '_' not in _id:\n",
    "        out_path = TE_path_dict['SSR']\n",
    "    out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "    result = REPET_ID_bed.filter(id_filter, _id).saveas(out_fn)\n",
    "    cov_fn = out_fn.replace('gff','cov')\n",
    "    cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "    cov.saveas(cov_fn)\n",
    "    #_len = len(pd.read_csv(cov_fn, header=None, sep='\\t'))\n",
    "    #_dict[_id] = _len\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a loop that takes in the filtered id'ed gff file, filters the file according to the id field. The corresponding bedfile is saved in a specific folder for the type of TE (RNA, DNA, SSR, noCat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TE_types = ['Retrotransposon', 'DNA_transposon', 'noCat', 'SSR']\n",
    "TE_path = [os.path.join(out_dir, x) for x in TE_types]\n",
    "TE_path_dict = dict(zip(TE_types, TE_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for TE_type in TE_types:\n",
    "    new_path = os.path.join(out_dir, TE_type)\n",
    "    if not os.path.exists(new_path):\n",
    "        os.mkdir(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_id = pd.read_csv(out_dir+'/'+genome+'.REPET.ID_column.gff', header=None, sep='\\t')[8].unique()\n",
    "REPET_ID_bed = pybedtools.BedTool(out_dir+'/'+genome+'.REPET.ID_column.gff')\n",
    "REPET_ID_bed = REPET_ID_bed.remove_invalid().saveas(out_dir+'/'+genome+'.REPET.ID_column.bedobject')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "\n",
    "jobs = []\n",
    "bed_file = g\n",
    "total_jobs = len(_id)\n",
    "job_count = 0\n",
    "manager = multiprocessing.Manager()\n",
    "TE_cov_df  = manager.dict()\n",
    "while job_count < total_jobs:\n",
    "    while [j.is_alive() for j in jobs].count(True) > 50:\n",
    "        time.sleep(15)\n",
    "        print(\"Waiting for Subset_id_classification to finish! More than 10 jobs running\")\n",
    "        print('%s jobs started' % job_count)\n",
    "    if job_count +10 < total_jobs:\n",
    "        for i in range(job_count, job_count+10):\n",
    "            p = multiprocessing.Process(target=subset_id, args=([_id[i],TE_cov_df]))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            job_count += 1\n",
    "            \n",
    "    else:\n",
    "        for i in range(job_count, total_jobs+1):\n",
    "            p = multiprocessing.Process(target=subset_id, args=([_id[i], TE_cov_df]))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            job_count += 1\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done! Totaling %i\"% (job_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(job_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id_q(_id_list, out_q):\n",
    "    outdict ={}\n",
    "    repet_prefix = 'Pst_104E_v12_p_ctg.REPET.TE_level'\n",
    "    for _id in _id_list:\n",
    "    \n",
    "        #ClassI are retrotransposon form blast\n",
    "        if 'ClassI:' in _id:\n",
    "            out_path = TE_path_dict['Retrotransposon']   \n",
    "        #ClassII are DNA_transponson\n",
    "        elif 'ClassII' in _id:\n",
    "            out_path = TE_path_dict['DNA_transposon'] \n",
    "        #The rest with '_' should be REPET_TEs\n",
    "        elif _id.split('_')[0] in list(code_dict.keys()):\n",
    "            key = code_dict[_id.split('_')[0]].split(' ')[0]\n",
    "            out_path = TE_path_dict[key]\n",
    "        #everything without '_' at the end should be SSR\n",
    "        elif '_' not in _id:\n",
    "            out_path = TE_path_dict['SSR']\n",
    "        out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "        result = REPET_ID_bed.filter(id_filter, _id).saveas(out_fn)\n",
    "        cov_fn = out_fn.replace('gff','cov')\n",
    "        cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "        cov.saveas(cov_fn)\n",
    "        _len = len(pd.read_csv(cov_fn, header=None, sep='\\t'))\n",
    "        outdict[_id] = _len\n",
    "    out_q.put(outdict)\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def mp_factorizer(nums, nprocs):\n",
    "    def worker(nums, out_q):\n",
    "        \"\"\" The worker function, invoked in a process. 'nums' is a\n",
    "            list of numbers to factor. The results are placed in\n",
    "            a dictionary that's pushed to a queue.\n",
    "        \"\"\"\n",
    "        outdict = {}\n",
    "        for n in nums:\n",
    "            outdict[n] = factorize_naive(n)\n",
    "        out_q.put(outdict)\n",
    "\n",
    "    # Each process will get 'chunksize' nums and a queue to put his out\n",
    "    # dict into\n",
    "    out_q = Queue()\n",
    "    chunksize = int(math.ceil(len(nums) / float(nprocs)))\n",
    "    procs = []\n",
    "\n",
    "    for i in range(nprocs):\n",
    "        p = multiprocessing.Process(\n",
    "                target=worker,\n",
    "                args=(nums[chunksize * i:chunksize * (i + 1)],\n",
    "                      out_q))\n",
    "        procs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    # Collect all results into a single result dict. We know how many dicts\n",
    "    # with results to expect.\n",
    "    resultdict = {}\n",
    "    for i in range(nprocs):\n",
    "        resultdict.update(out_q.get())\n",
    "\n",
    "    # Wait for all worker processes to finish\n",
    "    for p in procs:\n",
    "        p.join()\n",
    "\n",
    "    return resultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use multiprocessing to do the bedcov genome coverage per classification. Keep track if everything is already done.\n",
    "import math\n",
    "out_q = multiprocessing.Queue()\n",
    "nproces = 30 #number of processors\n",
    "chunksize = int(math.ceil(len(_id) / float(nproces)))\n",
    "jobs = []\n",
    "job_count = 0\n",
    "#manager = multiprocessing.Manager()\n",
    "#TE_cov_df  = manager.dict()\n",
    "for i in range(nproces):\n",
    "        p = multiprocessing.Process(target=subset_id_q, args=(_id[chunksize * i:chunksize * (i + 1)],out_q))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        job_count += 1\n",
    "# Collect all results into a single result dict. We know how many dicts\n",
    "# with results to expect.\n",
    "resultdict = {}\n",
    "for i in range(nproces):\n",
    "    resultdict.update(out_q.get())\n",
    "\n",
    "# Wait for all worker processes to finish\n",
    "for p in jobs:\n",
    "    p.join()\n",
    "\n",
    "        \n",
    "        \n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done! Totaling %i\"% (job_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = multiprocessing.Pool(20)\n",
    "p.map(subset_id, _id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset the id and safe in specific folder\n",
    "# return the subsetted file as bedtool\n",
    "def subset_id_m(_id,):\n",
    "    repet_prefix = 'Pst_104E_v12_p_ctg.REPET.TE_level'\n",
    "    #ClassI are retrotransposon form blast\n",
    "    if 'ClassI:' in _id:\n",
    "        out_path = TE_path_dict['Retrotransposon']   \n",
    "    #ClassII are DNA_transponson\n",
    "    elif 'ClassII' in _id:\n",
    "        out_path = TE_path_dict['DNA_transposon'] \n",
    "    #The rest with '_' should be REPET_TEs\n",
    "    elif _id.split('_')[0] in list(code_dict.keys()):\n",
    "        key = code_dict[_id.split('_')[0]].split(' ')[0]\n",
    "        out_path = TE_path_dict[key]\n",
    "    #everything without '_' at the end should be SSR\n",
    "    elif '_' not in _id:\n",
    "        out_path = TE_path_dict['SSR']\n",
    "    out_fn = out_path+'/'+repet_prefix+'.'+_id+'.gff'\n",
    "    result = REPET_ID_bed.filter(id_filter, _id).saveas(out_fn)\n",
    "    cov_fn = out_fn.replace('gff','cov')\n",
    "    cov = result.genome_coverage(dz=True,g=p_genome_file)\n",
    "    cov.saveas(cov_fn)\n",
    "    #_len = len(pd.read_csv(cov_fn, header=None, sep='\\t'))\n",
    "    #_dict[_id] = _len\n",
    "    #return pybedtools.BedTool(result.fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "jobs = []\n",
    "bed_file = g\n",
    "total_jobs = len(_id)\n",
    "job_count = 0\n",
    "manager = multiprocessing.Manager()\n",
    "TE_cov_df  = manager.dict()\n",
    "\n",
    "    while [j.is_alive() for j in jobs].count(True) > 10:\n",
    "        time.sleep(15)\n",
    "        print(\"Waiting for Subset_id_classification to finish! More than 10 jobs running\")\n",
    "        print('%s jobs started' % job_count)\n",
    "    if job_count +10 < total_jobs:\n",
    "        for i in range(job_count, job_count+10):\n",
    "            p = multiprocessing.Process(target=subset_id, args=([_id[i],TE_cov_df]))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            job_count += 1\n",
    "            \n",
    "    else:\n",
    "        for i in range(job_count, total_jobs+1):\n",
    "            p = multiprocessing.Process(target=subset_id, args=([_id[i], TE_cov_df]))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            job_count += 1\n",
    "while set([j.is_alive() for j in jobs]) != {False}:\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for Subset_id_classification to finish!\")\n",
    "print(\"All Subset_id_classifications done! Totaling %i\"% (job_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in TE_cov_df.keys():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[j.is_alive() for j in jobs].count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is REALLY slow for now. Would need to parallize this step. Look at the pool function of \n",
    "#multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[subset_id(x) for x in _id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this caputures all REPET cov files for each indiviual TE.\n",
    "class_cov_files = []\n",
    "for dirpath, dirname, filenames in os.walk(cur_dir):\n",
    "    #print(dirpath)\n",
    "    #print(len(filenames))\n",
    "    if len(filenames) == 0:  # empty folder\n",
    "        continue\n",
    "    cov_files = [dirpath +'/'+x for x in filenames if x.endswith('.cov') and 'classification' not in x ]\n",
    "    for file in cov_files:\n",
    "        class_cov_files.append(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(cov_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#make a large summary dataframe from all the cov files where the last \n",
    "df_list =[]\n",
    "class_cov_files.sort()\n",
    "for file in class_cov_files:\n",
    "    tmp_df = pd.read_csv(file, sep='\\t', header = None)\n",
    "    tmp_df[\"Code long\"] = file.split('.')[-2]\n",
    "    tmp_df.drop_duplicates(inplace=True)\n",
    "    df_list.append(tmp_df)\n",
    "    print(file.split('.')[-2])\n",
    "\n",
    "df_REPET_noblast_classification = pd.concat(df_list)\n",
    "\n",
    "cov_per_class_noblast = df_REPET_noblast_classification.pivot_table(values=1, columns= 'Code long', aggfunc='count')\n",
    "cov_per_contig_per_class_noblast = df_REPET_noblast_classification.groupby([0, 'Code long'])[1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
