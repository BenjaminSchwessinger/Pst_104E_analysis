{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is to read in the .tsv file of interproscan and pull out the following when having run interproscan with the following option.\n",
    "This was run like:\n",
    "$INTPRO/interproscan.sh -i ../../../v91_cns_gcoords_curs_ph_ctg_p_ctg.evm.all.protein.fa -iprlookup -goterms -pa\n",
    "and produced following file\n",
    "v91_cns_gcoords_curs_ph_ctg_p_ctg.evm.all.protein.intpro.fa.tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "import pysam\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from pybedtools import BedTool\n",
    "import numpy as np\n",
    "import pybedtools\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_FOLDER = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/funnotate/Pst_79a/intpro'\n",
    "INTERPRO_TSV_FILE = 'v91_cns_gcoords_cursf_ph_ctg_a_ctg.evm.all.protein.intpro.fa.tsv'\n",
    "BASE_AA_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/Pst_104E_v12'\n",
    "BASE_A_PATH = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/032017_assembly'\n",
    "ANNOTATION_FILE_BASE_PATH ='/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/funnotate/Pst_79a/fun-out/annotate_misc'\n",
    "OUT_PATH = os.path.join(BASE_AA_PATH, 'protein_annotation')\n",
    "if not os.path.exists(OUT_PATH):\n",
    "    os.mkdir(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_genome = 'Pst_104E_v12_h_ctg'\n",
    "OUT_PATH_P = os.path.join(OUT_PATH, p_genome)\n",
    "if not os.path.exists(OUT_PATH_P):\n",
    "    os.mkdir(OUT_PATH_P)\n",
    "#that is the path for all the proteins without removing the high coverage contigs and the proteins w/ \n",
    "#similarities to TE proteins\n",
    "OUT_PATH_P_ALL = os.path.join(OUT_PATH_P, 'ALL')\n",
    "if not os.path.exists(OUT_PATH_P_ALL):\n",
    "    os.mkdir(OUT_PATH_P_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull out all proteins that are in the final assembly\n",
    "p_protein_list = []\n",
    "protein_fa_file = [x for x in os.listdir(BASE_A_PATH) if p_genome in x and x.endswith('anno.protein.fa')][0]\n",
    "for protein in SeqIO.parse(os.path.join(BASE_A_PATH, protein_fa_file) , 'fasta'):\n",
    "    p_protein_list.append(protein.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpro_header = ['Protein_ID' , 'MD5', 'Length', 'DB', 'DB_accession', 'DB_description', 'Start_position', 'Stop_position', \\\n",
    "                  'e-value', 'Match Status', 'date', 'InterPro_ID', 'InterPro_description','GO_terms', 'Pathway_IDs' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpro_df = pd.read_csv(os.path.join(BASE_FOLDER, INTERPRO_TSV_FILE), sep ='\\t', header=None, names=interpro_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename protein names depending on the genome h or p\n",
    "def protein_id_conversion(old_protein_df):\n",
    "    '''Function that converts a old protein ID from v91 version to Pst_104E_v12 version. \n",
    "    E.g. \tevm.model.000004F_quiver.189 to evm.model.pcontig_004.189.\n",
    "    Input is a pandas series and output is a pandas series of same length.\n",
    "    '''\n",
    "    if p_genome.endswith('p_ctg'):\n",
    "        rename_df = old_protein_df.Protein_ID.str.extract(r'000([0-9]*)F_quiver.([0-9]*)')\n",
    "        rename_df['Updated_Protein_ID'] = 'evm.model.pcontig_' + rename_df[0] + '.' + rename_df[1]\n",
    "    elif p_genome.endswith('h_ctg'):\n",
    "        rename_df = old_protein_df.Protein_ID.str.extract(r'000([0-9]*)F_q?u?i?v?e?r?_?0?0?0?([0-9]{3})F?_quiver.([0-9]*)')\n",
    "        \n",
    "        rename_df['Updated_Protein_ID'] = 'evm.model.hcontig_' + rename_df[0] + '_' + rename_df[1] +'.' + rename_df[2]\n",
    "    return rename_df['Updated_Protein_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:11: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "interpro_df['Updated_Protein_ID'] = protein_id_conversion(interpro_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15335"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interpro_df.Protein_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14321"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p_protein_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpro_df.GO_terms.fillna(0, inplace = True)\n",
    "interpro_df.Pathway_IDs.fillna(0, inplace =True)\n",
    "interpro_df.InterPro_ID.fillna(0,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interpro_by_protein = interpro_df.groupby('Updated_Protein_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DBs = interpro_df.DB.unique()\n",
    "interpro_df['Dbxref'] = 'Dbxref'\n",
    "for db in DBs:\n",
    "    interpro_df[(interpro_df.DB == db) & (interpro_df.Updated_Protein_ID.isin(p_protein_list))].loc[:,['Updated_Protein_ID', 'DB_accession']]\\\n",
    "    .to_csv(os.path.join(OUT_PATH_P, db+'_terms.tab'), sep='\\t', header =None, index = None)\n",
    "    interpro_df[(interpro_df.DB == db) & (interpro_df.Updated_Protein_ID.isin(p_protein_list))].loc[:,['Updated_Protein_ID', 'Dbxref','DB_accession']]\\\n",
    "    .to_csv(os.path.join(OUT_PATH_P, 'annotations.' +db+'.txt'), sep='\\t', header =None, index = None)\n",
    "    interpro_df[(interpro_df.DB == db) ].loc[:,['Updated_Protein_ID', 'DB_accession']]\\\n",
    "    .to_csv(os.path.join(OUT_PATH_P_ALL, db+'_terms_all.tab'), sep='\\t', header =None, index = None)\n",
    "    interpro_df[(interpro_df.DB == db) ].loc[:,['Updated_Protein_ID', 'Dbxref','DB_accession']]\\\n",
    "    .to_csv(os.path.join(OUT_PATH_P_ALL, 'annotations.' +db+'_all.txt'), sep='\\t', header =None, index = None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write out the interpro domains\n",
    "interpro_df['InterPro'] = 'InterPro'\n",
    "interpro_df[(interpro_df.InterPro_ID != 0) & (interpro_df.Updated_Protein_ID.isin(p_protein_list))].loc[:,['Updated_Protein_ID', 'InterPro_ID']].to_csv(os.path.join(OUT_PATH_P, 'iprscan_terms.tab'), sep='\\t', header =None, index = None)\n",
    "interpro_df[interpro_df.InterPro_ID != 0].loc[:,['Updated_Protein_ID', 'InterPro_ID']].to_csv(os.path.join(OUT_PATH_P_ALL, 'iprscan_terms_all.tab'), sep='\\t', header =None, index = None)\n",
    "#write out annotations\n",
    "interpro_df[(interpro_df.InterPro_ID != 0) & (interpro_df.Updated_Protein_ID.isin(p_protein_list))]\\\n",
    ".loc[:,['Updated_Protein_ID', 'InterPro','InterPro_ID']].to_csv(os.path.join(OUT_PATH_P, 'annotations.iprscan.txt'), sep='\\t', header =None, index = None)\n",
    "#write out annotations for all proteins\n",
    "interpro_df[interpro_df.InterPro_ID != 0].loc[:,['Updated_Protein_ID', 'InterPro','InterPro_ID']].to_csv(os.path.join(OUT_PATH_P_ALL, 'annotations.iprscan_all.tab'), sep='\\t', header =None, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#process the pathway files\n",
    "interpro_by_protein_KEGG = interpro_df[interpro_df.Pathway_IDs.str.contains('KEGG' or 'MetaCyc' or 'Reactome').fillna(False)]\n",
    "\n",
    "interpro_by_protein_KEGG = interpro_by_protein_KEGG.groupby('Updated_Protein_ID')\n",
    "\n",
    "#pull out all the KEGG terms and write them out as annotation files \n",
    "interpro_by_protein_KEGG = interpro_by_protein.Pathway_IDs.apply(set)\n",
    "\n",
    "#remove everything without KEGG term attached\n",
    "interpro_by_protein_KEGG = interpro_by_protein_KEGG[~(interpro_by_protein_KEGG == {0})]\n",
    "\n",
    "interpro_by_protein_KEGG_dict = dict(zip(interpro_by_protein_KEGG.index, interpro_by_protein_KEGG))\n",
    "\n",
    "ALL_KEGG_LIST = []\n",
    "ALL_PROTEIN_INDEX_LIST = []\n",
    "for key in list(interpro_by_protein_KEGG_dict.keys()):\n",
    "    _tmp_list = list(interpro_by_protein_KEGG_dict[key])\n",
    "    #remove 0 \n",
    "    _tmp_list = [x for x in _tmp_list if x != 0]\n",
    "    new_value = []\n",
    "    for x in _tmp_list: \n",
    "        if '|' in x:\n",
    "            _list = x.split('|')\n",
    "            for y in _list:\n",
    "                new_value.append(y)\n",
    "        else:\n",
    "            new_value.append(x)\n",
    "    new_value = list(set(new_value))\n",
    "    new_index = [key]*len(new_value)\n",
    "    ALL_KEGG_LIST += new_value\n",
    "    ALL_PROTEIN_INDEX_LIST += new_index\n",
    "\n",
    "\n",
    "KEGG_df = pd.DataFrame([ALL_PROTEIN_INDEX_LIST, ALL_KEGG_LIST]).T\n",
    "KEGG_df.rename(columns={0:'Updated_Protein_ID', 1:'DB_ID'}, inplace=True)\n",
    "\n",
    "KEGG_df.to_csv(os.path.join(OUT_PATH_P_ALL, 'Pathway_terms_ipr_all.tab') , sep = '\\t', header =None, index=None)\n",
    "KEGG_df[KEGG_df.Updated_Protein_ID.isin(p_protein_list)].to_csv(os.path.join(OUT_PATH_P, 'Pathway_terms_ipr.tab') , sep = '\\t', header =None, index=None)\n",
    "#write out annotations\n",
    "KEGG_df['Transfer_ID'] = 'note'\n",
    "KEGG_df.loc[:,['Updated_Protein_ID', 'Transfer_ID','DB_ID']].to_csv(os.path.join(OUT_PATH_P_ALL, 'annotations.Pathway_all.txt') , sep = '\\t', header =None, index=None)\n",
    "KEGG_df[KEGG_df.Updated_Protein_ID.isin(p_protein_list)].loc[:,['Updated_Protein_ID', 'Transfer_ID','DB_ID']].to_csv(os.path.join(OUT_PATH_P, 'annotations.Pathway.txt') , sep = '\\t', header =None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull out all the GO terms and write them out as annotation files \n",
    "interpro_by_protein_GO = interpro_by_protein.GO_terms.apply(set)\n",
    "\n",
    "#remove everything without GO term attached\n",
    "interpro_by_protein_GO = interpro_by_protein_GO[~(interpro_by_protein_GO == {0})]\n",
    "\n",
    "interpro_by_protein_GO_dict = dict(zip(interpro_by_protein_GO.index, interpro_by_protein_GO))\n",
    "\n",
    "ALL_GO_LIST = []\n",
    "ALL_PROTEIN_INDEX_LIST = []\n",
    "for key in list(interpro_by_protein_GO_dict.keys()):\n",
    "    _tmp_list = list(interpro_by_protein_GO_dict[key])\n",
    "    #remove 0 \n",
    "    _tmp_list = [x for x in _tmp_list if x != 0]\n",
    "    new_value = []\n",
    "    for x in _tmp_list: \n",
    "        if '|' in x:\n",
    "            _list = x.split('|')\n",
    "            for y in _list:\n",
    "                new_value.append(y)\n",
    "        else:\n",
    "            new_value.append(x)\n",
    "    new_value = list(set(new_value))\n",
    "    new_index = [key]*len(new_value)\n",
    "    ALL_GO_LIST += new_value\n",
    "    ALL_PROTEIN_INDEX_LIST += new_index\n",
    "\n",
    "\n",
    "GO_df = pd.DataFrame([ALL_PROTEIN_INDEX_LIST, ALL_GO_LIST]).T\n",
    "GO_df.rename(columns={0:'Updated_Protein_ID', 1:'DB_ID'}, inplace=True)\n",
    "\n",
    "GO_df.to_csv(os.path.join(OUT_PATH_P_ALL, 'GO_terms_ipr_all.tab') , sep = '\\t', header =None, index=None)\n",
    "GO_df[GO_df.Updated_Protein_ID.isin(p_protein_list)].to_csv(os.path.join(OUT_PATH_P, 'GO_terms_ipr.tab') , sep = '\\t', header =None, index=None)\n",
    "#write out annotations\n",
    "GO_df['Transfer_ID'] = 'note'\n",
    "GO_df.loc[:,['Updated_Protein_ID', 'Transfer_ID','DB_ID']].to_csv(os.path.join(OUT_PATH_P_ALL, 'annotations.GO_all.txt') , sep = '\\t', header =None, index=None)\n",
    "GO_df[GO_df.Updated_Protein_ID.isin(p_protein_list)].loc[:,['Updated_Protein_ID', 'Transfer_ID','DB_ID']].to_csv(os.path.join(OUT_PATH_P, 'annotations.GO.txt') , sep = '\\t', header =None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter out other annotations files including busco, dbCAN, merops, swissprot\n",
    "annotation_files = [os.path.join(ANNOTATION_FILE_BASE_PATH, x) for x in os.listdir(ANNOTATION_FILE_BASE_PATH) \\\n",
    "                    if x.startswith('annotations') and (x.endswith('busco.txt') or x.endswith('dbCAN.txt') or x.endswith('merops.txt') or x.endswith('swissprot.txt'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:11: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "for anno in annotation_files:\n",
    "    _tmp_df = pd.read_csv(anno, header=None, sep='\\t', names=['Protein_ID','Transfer_ID', 'DB_ID'])\n",
    "    anno_file_name = anno.split('/')[-1]\n",
    "    anno_midfix = anno_file_name.split('.')[1]\n",
    "    _tmp_df['Updated_Protein_ID'] = protein_id_conversion(_tmp_df)\n",
    "    #write out filtered down tab file\n",
    "    _tmp_df[(_tmp_df.Updated_Protein_ID.isin(p_protein_list))].loc[:,['Updated_Protein_ID', 'DB_ID']].to_csv(os.path.join(OUT_PATH_P, anno_midfix+'_terms.tab'), sep = '\\t', header =None, index=None)\n",
    "    #write out the filtered annotation file\n",
    "    _tmp_df[(_tmp_df.Updated_Protein_ID.isin(p_protein_list))].loc[:,['Updated_Protein_ID', 'Transfer_ID','DB_ID']].\\\n",
    "    to_csv(os.path.join(OUT_PATH_P, anno_file_name ), sep = '\\t', header =None, index=None)\n",
    "     #write out not down tab file\n",
    "    _tmp_df.loc[:,['Updated_Protein_ID', 'DB_ID']].to_csv(os.path.join(OUT_PATH_P_ALL, anno_midfix+'_terms_all.tab' ), sep = '\\t', header =None, index=None)\n",
    "    #write out the filtered annotation file\n",
    "    _tmp_df.loc[:,['Updated_Protein_ID', 'Transfer_ID','DB_ID']].to_csv(os.path.join(OUT_PATH_P_ALL, anno_file_name.replace('.txt', '_all.txt') ), sep = '\\t', header =None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benjamin/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:11: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "for anno in annotation_files[-1:]:\n",
    "    _tmp_df = pd.read_csv(anno, header=None, sep='\\t', names=['Protein_ID','Transfer_ID', 'DB_ID'])\n",
    "    anno_file_name = anno.split('/')[-1]\n",
    "    anno_midfix = anno_file_name.split('.')[1]\n",
    "    _tmp_df['Updated_Protein_ID'] = protein_id_conversion(_tmp_df)\n",
    "    #write out filtered down tab file\n",
    "    _tmp_df[(_tmp_df.Updated_Protein_ID.isin(p_protein_list))].loc[:,['Updated_Protein_ID', 'DB_ID']].to_csv(os.path.join(OUT_PATH_P, anno_midfix+'_terms.tab'), sep = '\\t', header =None, index=None)\n",
    "    #write out the filtered annotation file\n",
    "    _tmp_df[(_tmp_df.Updated_Protein_ID.isin(p_protein_list))].loc[:,['Updated_Protein_ID', 'Transfer_ID','DB_ID']].\\\n",
    "    to_csv(os.path.join(OUT_PATH_P, anno_file_name ), sep = '\\t', header =None, index=None)\n",
    "     #write out not down tab file\n",
    "    _tmp_df.loc[:,['Updated_Protein_ID', 'DB_ID']].to_csv(os.path.join(OUT_PATH_P_ALL, anno_midfix+'_terms_all.tab' ), sep = '\\t', header =None, index=None)\n",
    "    #write out the filtered annotation file\n",
    "    _tmp_df.loc[:,['Updated_Protein_ID', 'Transfer_ID','DB_ID']].to_csv(os.path.join(OUT_PATH_P_ALL, anno_file_name.replace('.txt', '_all.txt') ), sep = '\\t', header =None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOG092R007J\tFragmented\tevm.model.pcontig_031.21\t219.6\t156\n",
      "EOG092R007J\tFragmented\tevm.model.pcontig_031.21\t219.6\t156\n",
      "EOG092R03FK\tMissing\n",
      "EOG092R051W\tMissing\n",
      "EOG092R052F\tDuplicated\tevm.model.hcontig_016_024.68\t112.3\t220\n",
      "EOG092R052F\tDuplicated\tevm.model.pcontig_016.82\t112.3\t220\n",
      "EOG092R0BH2\tMissing\n",
      "EOG092R0H00\tDuplicated\tevm.model.hcontig_022_007.40\t312.5\t273\n",
      "EOG092R0H00\tDuplicated\tevm.model.pcontig_022.130\t312.5\t273\n",
      "EOG092R0H00\tDuplicated\tevm.model.pcontig_185.4\t312.5\t273\n",
      "EOG092R0H6Z\tMissing\n",
      "EOG092R0KI1\tMissing\n",
      "EOG092R0KL3\tMissing\n",
      "EOG092R0Q2F\tMissing\n",
      "EOG092R0QEF\tComplete\tevm.model.hcontig_074_001.8\t246.7\t327\n",
      "Now haplotype specific file\n",
      "EOG092R007J\tDuplicated\tevm.model.000031F_003_quiver.3\t748.6\t515\n",
      "EOG092R007J\tDuplicated\tevm.model.000166F_001_quiver.3\t748.6\t515\n",
      "EOG092R007J\tDuplicated\tevm.model.000031F_003_quiver.3\t748.6\t515\n",
      "EOG092R007J\tDuplicated\tevm.model.000166F_001_quiver.3\t748.6\t515\n",
      "EOG092R03FK\tComplete\tevm.model.000002F_028_quiver.219\t839.8\t521\n",
      "EOG092R051W\tComplete\tevm.model.000064F_007_quiver.17\t567.2\t400\n",
      "EOG092R052F\tComplete\tevm.model.000010F_016_quiver.107\t483.7\t338\n",
      "EOG092R0BH2\tComplete\tevm.model.000045F_002_quiver.36\t531.7\t509\n",
      "EOG092R0H00\tDuplicated\tevm.model.000022F_007_quiver.40\t312.5\t273\n",
      "EOG092R0H00\tDuplicated\tevm.model.000185F_001_quiver.4\t312.5\t273\n",
      "EOG092R0H6Z\tComplete\tevm.model.000041F_004_quiver.34\t490.8\t302\n",
      "EOG092R0KI1\tComplete\tevm.model.000006F_013_quiver.48\t362.2\t171\n",
      "EOG092R0KL3\tComplete\tevm.model.000009F_029_quiver.15\t216.4\t190\n",
      "EOG092R0Q2F\tComplete\tevm.model.000077F_002_quiver.20\t279.3\t140\n",
      "EOG092R0QEF\tDuplicated\tevm.model.000027F_006_quiver.11\t244.5\t393\n",
      "EOG092R0QEF\tDuplicated\tevm.model.000074F_001_quiver.8\t246.7\t385\n"
     ]
    }
   ],
   "source": [
    "#realized that some of the BUSCOs were actually filtered out at the TE filtereing step.\n",
    "BUSCO_FILE_NAME_pa = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/busco/Pst79_pa/run_v91_cns_gcoords_curs_ph_ctg_pa_ctg.protein/full_table_v91_cns_gcoords_curs_ph_ctg_pa_ctg.protein.tsv'\n",
    "BUSCO_FILE_NAME_specific = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/busco/Pst79_a/run_v91_cns_gcoords_cursf_ph_ctg_a_ctg.protein/full_table_v91_cns_gcoords_cursf_ph_ctg_a_ctg.protein.tsv'\n",
    "BUSCO_FINAL_FILE = '/home/benjamin/genome_assembly/PST79/FALCON/p_assemblies/v9_1/busco/Pst_104E_v12/run_Pst_104E_v12_ph_ctg.anno.protein.fa.protein/full_table_Pst_104E_v12_ph_ctg.anno.protein.fa.protein.tsv'\n",
    "for x in _tmp_df[~(_tmp_df.Updated_Protein_ID.isin(p_protein_list))].iloc[:,2].str.split(':'):\n",
    "    #!grep -w '{x[1]}' {BUSCO_FILE_NAME_pa} | cut -f 1 | xargs -I X grep -w 'X' {BUSCO_FINAL_FILE}\n",
    "    !grep -w '{x[1]}' {BUSCO_FINAL_FILE}\n",
    "print('Now haplotype specific file')\n",
    "for x in _tmp_df[~(_tmp_df.Updated_Protein_ID.isin(p_protein_list))].iloc[:,2].str.split(':'):\n",
    "    !grep -w '{x[1]}' {BUSCO_FILE_NAME_specific} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#non of those BUSCOS were in the final file BUSCO_FINAL_FILE yet only 19 BUSCOs are missing\n",
    "#compared to 10. It appears that in some cases one of the BUSCO hits was filtered out put not the other.\n",
    "#some BUSCOs got second rate hits that turned fragmented to complete and where not filter against TE database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is such a case. here hcontig_185 was actually reverted to a primary contig as its own got lost.\n",
    "'evm.model.hcontig_022_007.40' in _tmp_df.Updated_Protein_ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evm.model.000022F_007_quiver.40\tnote\tBUSCO:EOG092R0H00\r\n"
     ]
    }
   ],
   "source": [
    "!grep 'evm.model.000022F_007_quiver.40' {anno}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
